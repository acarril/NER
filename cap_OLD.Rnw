<<cap_OLD, echo=FALSE, cache=FALSE>>=
set_parent('NER.Rnw')
@

\chapter{OLD}

\hrulefill

El modelo de regresión lineal es el caballo de batalla de la econometría aplicada, y puede escribirse como
\begin{equation}
y_i = x_i^T \beta + \epsilon_i,
\end{equation}
donde $i = 1, \ldots, n$ es un índice que identifica a las observaciones (filas) en los datos. Podríamos tener $n$ personas, familias o empresas.
También es común escribir el mismo modelo en forma matricial:
\begin{equation}
y = X \beta + \epsilon.
\end{equation}

En estas ecuaciones $y$ (o $y_i$) es un vector $n \times 1$ que representa la variable dependiente del modelo, es decir, la variable que queremos explicar. Por ejemplo, $y$ podría ser un vector de salario cuyos valores son los ingresos mensuales de $n$ personas.
La variable dependiente es explicada por un conjunto de $k$ variables independientes, usalmente denotadas por $X$. Entonces $X$ es una matriz $n\times k$, ya que contiene información de $k$ variables (columnas) para $n$ observaciones (filas).

Al estimar este tipo de modelos usualmente nos interesa determinar $\beta$, que representa la relación lineal entre $y$ y $X$, es decir, entre la variable dependiente y el conjunto de variables independientes que la explican. El vector $\beta$ contiene $k \times 1$ coeficientes de regresión, de forma que estimaremos un coeficiente por cada variable incluída en $X$.
Finalmente $\epsilon$ es el llamado término de error, un vector $n \times 1$ que captura las discrepancias para cada observación $i$ que resulta de imponer una relación lineal entre $X$ e $y$.

Para entender bien todo esto resulta muy útil manipular un modelo simple. Para esto usaremos \verb|sim1|, una base de datos simulada que contiene dos variables: \verb|x| e \verb|y|. Cargamos el paquete \paq{modelr} para poder acceder esta base de datos, y además cargamos \paq{ggplot2} para crear gráficos:

<<>>=
library(modelr)
library(ggplot2)
@

Primero que nada conviene imprimir el encabezado de la base de datos, que corresponde a las primeras observaciones.

<<eval=TRUE>>=
sim1
@

Esto es lo primero que debieses hacer antes de empezar a utilizar datos nuevos, ya que permite tener una idea rápida sobre ellos. Vemos que hay 30 observaciones ($n=30$) y que \verb|x| solo toma valores enteros (y hay valores repetidos), mientras que \verb|y| es continua.

Ahora graficamos estos datos para hacernos una idea de la relación que existe entre ambos:

<<>>=
ggplot(sim1, aes(x, y)) + geom_point()
@

Oh sorpresa, los datos simulados presentan una clara correlación positiva. Parece casi diseñado para ser modelado linealmente...
%La pregunta relevante, sin embargo, es cuál línea usar.
Recordemos que en este caso nuestro modelo lineal puede escribirse simplemente como

\begin{equation*}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i 
\end{equation*}

Entonces la pregunta relevante es ``¿qué línea es la que mejor captura la relación entre $x$ e $y$?''.
Notemos que en este caso bidimensional simple $\beta_0$ y $\beta_1$ representan el intercepto y la pendiente de la recta, respectivamente.
Por lo tanto la pregunta también puede plantearse como: ¿cuáles son los valores de $\beta_0$ y $\beta_1$ que describen mejor esta relación?


<<echo=FALSE>>=
models <- data.frame(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)
ggplot(sim1, aes(x, y)) +
  geom_point() +
  geom_abline(
    aes(intercept = a1, slope = a2),
    data = models, alpha = 1/4
  )
@

\subsection{Mínimos Cuadrados Ordinarios}

El método de Mínimos Cuadrados Ordinarios (MCO) es una respuesta directa a la pregunta que planteamos recién: ¿qué criterio conviene usar para elegir el modelo lineal que se ajuste ``mejor'' a los datos?
Lo que propone MCO es minimizar la suma de los errores al cuadrado.
Geométricamente, esto equivale a minimizar la distancia vertical entre los puntos y la recta definida por el modelo.

<<echo=FALSE, message=FALSE>>=
library(dplyr)
dist1 <- sim1 %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge,
    pred = 7 + x1 * 1.5
  )

ggplot(dist1, aes(x1, y)) + 
  geom_abline(intercept = 7, slope = 1.5, colour = "grey40") +
  geom_point(colour = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), colour = "blue")
@

La distancia entre un puntos y la recta el error de predicción del modelo, capturado por $\epsilon_i$. Es decir que para cada observación $i$ tenemos una medida de la magnitud del error que esta elección de $\beta_0$ y $\beta_1$ produce. El criterio de MCO entonces es elegir $\beta_0$ y $\beta_1$ tal que se minimice el promedio de todos los errores al cuadrado.\footnote{¿Por qué al cuadrado? Porque se asume que es irrelevante si el error es una subestimación o una sobreestimación, de forma que lo único que importa es la magnitud. ¿Por qué no se usa el valor absoluto entonces? Buena pregunta.}



\hrulefill
%Para esta sección usaremos el paquete \paq{AER}, que incluye varias bases de datos para econometría. Ahora usaremos \verb|AER::CPS1988|, que contiene información de salarios de EE.UU. en Marzo de 1988.

<<message=FALSE>>=
library(AER)
data("CPS1988")
@

Consideremos la vieja y confiable ecuación de salarios de Mincer:

\begin{equation}
\text{salario}_i = \beta_0 + \beta_1 \text{educación}_i + \beta_2 \text{experiencia}_i + \epsilon_i
\label{eq:mincer}
\end{equation}

En términos de la base \verb|CPS1988|, nos interesan por el momento las variables \verb|wage|, \verb|education| y \verb|experience|. Podemos estimar este modelo lineal simple escribiendo

<<>>=
lm(wage ~ education + experience, data = CPS1988)
@

El resultado que imprime \verb|lm()| es una versión muy abreviada de todo lo que fue calculado, ya que sólo entrega las estimaciones de los coeficientes $\beta$ del modelo. Para ver más detalles podemos guardar los resultados de \verb|lm()| en el objeto \verb|mincer| y luego aplicarle la función \verb|summary()|:

<<>>=
mincer <- lm(wage ~ education + experience, data = CPS1988)
summary(mincer)
@

Normalmente se usa la función \verb|summary()| en objetos de datos, tales comos vectores o marcos de datos, para obtener estadística descriptiva del objeto (promedio, desviación estándar, etc.). Al usarla en un objeto con resultados de una estimación ---como en este caso--- nos entregará información importantes de dicha estimación, como el $R^2$ del modelo o los errores estándar de los coeficientes.

Podemos extraer otro tipo de información y objetos del modelo. La función \verb|residuals()| entrega los residuos del modelo, y \verb|vcov()| entrega la matriz de varianzas-covarianzas de los coeficientes:

<<eval=FALSE>>=
# Residuos
residuals(mincer)
# Matriz de varianzas-covarianzas:
vcov(mincer)
@

Obviamente estas funciones sólo sirven para realizar otras computaciones y no para ser impresas directamente, y es por eso que he omitido su resultado.

Es común querer estimar un modelo sin intercepto, lo que se logra escribiendo el intercepto explícitamente en la fórmula, usualmente como 0:

<<>>=
lm(wage ~ 0 + education + experience, data = CPS1988)
@

Podemos especificar formas funcionales más complejas para nuestro modelo básico expresado en \eqref{eq:mincer}, incluyendo términos interactuados con \verb|*| y términos cuadráticos encerrados en la función \verb|I()|:

<<>>=
lm(wage ~ experience + I(experience^2) + education + education*parttime, 
   CPS1988)
@

También es posible aplicar a los vectores alguna transformación directamente, como el logaritmo o raíz cuadrada (con \verb|log()| y \verb|sqrt()|). Hay que ser cuidados con los valores que toman las variables a las que se aplican estas funciones, recordando que la raíz cuadrada no está definida para números negativos y que $\ln(0) \rightarrow -\infty$:

<<warning=FALSE>>=
sqrt(-1)
log(0)
@


\subsection{Extraer y calcular información de una regresión}

Un objeto de clase \verb|lm| ---como \verb|mincer|--- es una lista que contiene varios atributos. Estos atributos son objetos calculados por \verb|lm()| con información a la que podemos acceder. Para imprimir una lista completa de los atributos de un objeto usamos \verb|attributes()|:

<<>>=
attributes(mincer)
@

Para usar algún atributo necesitamos la notación de signo \verb|$|. Por ejemplo, podemos guardar el vector $\hat y$ con

<<>>=
yhat <- mincer$fitted.values
@

Esto puede ser útil, por ejemplo, al implementar el método de variables instrumentales en dos etapas, donde tenemos que rescatar los valores estimados $\hat x$ en la primera etapa para luego incluirlos en la ecuación original. Escribiendo \verb|?lm| puedes obtener más información sobre todos estos atributos.

Vimos que \verb|summary()| calcula e imprime otra información útil del resultado de una regresión. Análogo a \verb|lm()|, \verb|summary()| también tiene atributos a los que podemos acceder:

<<>>=
mincer.summ <- summary(mincer)
attributes(mincer.summ)
@

Usando estos atributos podemos calcular fácilmente algunos resultados útiles. Por ejemplo, podemos obtener la matriz de varianzas-covarianzas del modelo:

<<>>=
(mincer.summ$sigma)^2 * mincer.summ$cov.unscaled
@

Finalmente, además de los cálculos que podemos efectuar directamente con estos atributos, existen muchas funciones que toman un objeto de clase \verb|lm| y calculan directamente algún resultado. Por ejemplo, podríamos haber calculado la matriz de varianzas-covarianzas directamente con \verb|vcov()|:

<<>>=
vcov(mincer)
@

Otras funciones útiles para usar con este tipo de objetos son:

<<eval=FALSE>>=
# Calcular suma de los cuadrados de los residuos (SSR):
deviance(mincer)
# Extraer log-likelihood (LL):
logLik(mincer) # no tiene mucho sentido aquí porque no estimamos con MV
# Calcular el critero de información de Akaike (AIC):
AIC(mincer)
@

Notar que \verb|AIC()| define el criterio como
\begin{equation}
AIC = -2 \ln \widehat L(p) + K\cdot 2,
\end{equation}
donde $K$ es el número de parámetros estimados y $\widehat L(p)$ es la función de verosimilitud. En muchos casos se prefiere que el criterio sea $AIC/N$, es decir, usar el Criterio de Información Bayesiano (BIC). Esto es,
\begin{equation}
BIC = -2 \ln \widehat L(p) + K \cdot \ln N,
\end{equation}
donde $N$ es el número de observaciones.
Para calcular el BIC también usamos \verb|AIC()|, pero especificando un parámetro de penalización distinto:
<<>>=
AIC(mincer, k = log(NROW(CPS1988))) # fijar k=2 es igual que el AIC clásico
@


\subsection{Heterocedasticidad}

La heterocedasticidad es un problema que es más difícil de pronunciar que de entender: ocurre cuando la dispersión de una variable dependiente no es constante para distintos valores de la variable independiente.

La \autoref{fig:heterocedasticidad} muestra datos heterocedásticos para el caso de una regresión bivariada, tanto para una variable dependiente discreta como continua.
En el eje $f(\mu)$ se grafica la densidad del término de error, la que claramente cambia (aumenta) a medida que $x$ es mayor.

\begin{figure}[htb]
\pgfmathsetseed{112}
\centering
\begin{subfigure}{.5\textwidth}
\makeatletter
        \pgfdeclareplotmark{dot}
        {%
            \fill circle [x radius=0.08, y radius=0.32];
        }%
\makeatother
  \centering
  \begin{tikzpicture}[ % Define Normal Probability Function
declare function={
            normal(\x,\m,\s) = 1/(2*\s*sqrt(pi))*exp(-(\x-\m)^2/(2*\s^2));
        },
    declare function={invgauss(\a,\b) = sqrt(-2*ln(\a))*cos(deg(2*pi*\b));}
       ]
\begin{axis}[
    %no markers,
    domain=0:12,
    zmin=0, zmax=1,
    xmin=0, xmax=3,
    samples=200,
   samples y=0,
    view={40}{30},
    axis lines=middle,
    enlarge y limits=false,
    xtick={0.5,1.5,2.5},
    xmajorgrids,
    xticklabels={},
    ytick=\empty,
%   xticklabels={$x_1$, $x_2$, $x_3$},
    ztick=\empty,
    xlabel=$x$, xlabel style={at={(rel axis cs:1,0,0)}, anchor=west},
    ylabel=$y$, ylabel style={at={(rel axis cs:0,1,0)}, anchor=south west},
    zlabel=$f(\mu)$, zlabel style={at={(rel axis cs:0,0,1)}, anchor=south},
    set layers, mark=cube
  ]

\pgfplotsinvokeforeach{0.5,1.5,2.5}{
\addplot3 [draw=none, fill=black, opacity=0.25, only marks, mark=dot, mark layer=like plot, samples=30, domain=0.1:2.9, on layer=axis background] (#1, {1.5*(#1-0.5)+3+invgauss(rnd,rnd)*#1}, 0);
}
\addplot3 [samples=2, samples y=0, domain=0:3] (x, {1.5*(x-0.5)+3}, 0);
\addplot3 [blue!50, thick] (0.5, x, {normal(x, 3, 0.5)});
\addplot3 [blue!50, thick] (1.5, x, {normal(x, 4.5, 1)});
\addplot3 [blue!50, thick] (2.5, x, {normal(x, 6, 1.5)});

\pgfplotsextra{
\begin{pgfonlayer}{axis background}
\draw [gray, on layer=axis background] (0.5, 3, 0) -- (0.5, 3, {normal(0,0,0.5)}) (0.5,0,0) -- (0.5,12,0)
    (1.5, 4.5, 0) -- (1.5, 4.5, {normal(0,0,1)}) (1.5,0,0) -- (1.5,12,0)
    (2.5, 6, 0) -- (2.5, 6, {normal(0,0,1.5)}) (2.5,0,0) -- (2.5,12,0);

\end{pgfonlayer}
}
\end{axis}

\end{tikzpicture}
  \caption{Variable dependiente discreta}
  \label{fig:heterocedasticidad_x_discreta}
\end{subfigure}%
\makeatletter
        \pgfdeclareplotmark{dot}
        {%
            \fill circle [x radius=0.02, y radius=0.08];
        }%
\makeatother
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tikzpicture}[ % Define Normal Probability Function
declare function={
            normal(\x,\m,\s) = 1/(2*\s*sqrt(pi))*exp(-(\x-\m)^2/(2*\s^2));
        },
    declare function={invgauss(\a,\b) = sqrt(-2*ln(\a))*cos(deg(2*pi*\b));}
       ]
\begin{axis}[
    %no markers,
    domain=0:12,
    zmin=0, zmax=1,
    xmin=0, xmax=3,
    samples=200,
   samples y=0,
    view={40}{30},
    axis lines=middle,
    enlarge y limits=false,
    xtick={0.5,1.5,2.5},
    xmajorgrids,
    xticklabels={},
    ytick=\empty,
%   xticklabels={$x_1$, $x_2$, $x_3$},
    ztick=\empty,
    xlabel=$x$, xlabel style={at={(rel axis cs:1,0,0)}, anchor=west},
    ylabel=$y$, ylabel style={at={(rel axis cs:0,1,0)}, anchor=south west},
    zlabel=$f(\mu)$, zlabel style={at={(rel axis cs:0,0,1)}, anchor=south},
    set layers, mark=cube
  ]

\addplot3 [gray!50, only marks, mark=dot, mark layer=like plot, samples=200, domain=0.1:2.9, on layer=axis background] (x, {1.5*(x-0.5)+3+invgauss(rnd,rnd)*x}, 0);
\addplot3 [samples=2, samples y=0, domain=0:3] (x, {1.5*(x-0.5)+3}, 0);
\addplot3 [blue!50, thick] (0.5, x, {normal(x, 3, 0.5)});
\addplot3 [blue!50, thick] (1.5, x, {normal(x, 4.5, 1)});
\addplot3 [blue!50, thick] (2.5, x, {normal(x, 6, 1.5)});

\pgfplotsextra{
\begin{pgfonlayer}{axis background}
\draw [gray, on layer=axis background] (0.5, 3, 0) -- (0.5, 3, {normal(0,0,0.5)}) (0.5,0,0) -- (0.5,12,0)
    (1.5, 4.5, 0) -- (1.5, 4.5, {normal(0,0,1)}) (1.5,0,0) -- (1.5,12,0)
    (2.5, 6, 0) -- (2.5, 6, {normal(0,0,1.5)}) (2.5,0,0) -- (2.5,12,0);

\end{pgfonlayer}
}
\end{axis}


\end{tikzpicture}
  \caption{Variable dependiente continua}
  \label{fig:heterocedasticidad_x_continua}
\end{subfigure}
\caption{Heterocedasticidad}
\label{fig:heterocedasticidad}
\end{figure}



Podemos generar fácilmente un conjunto de datos que presenten el problema:

<<fig.width=4.5, fig.height=4.5>>=
set.seed(314)
n <- 256                           # Número de observaciones
x <- (1:n)/n                       # Valores de `x`
e <- rnorm(n, sd=1)                # Valores aleatorios de una normal
i <- order(runif(n, max=dnorm(e))) # Ubicamos los más grandes al final
y <- 1 + 5 * x + e[rev(i)]         # Generamos `y` con el error `e`.
modelo <- lm(y ~ x)                # Guardamos el modelo lineal
plot(x, y)
abline(coef(modelo), col = "Red")
@

En presencia de heterocedasticidad una estimación por MCO seguirá entregando coeficientes consistentes e insesgados. Sin embargo, no podremos estimar correctamente la matriz de varianzas-covarianzas, lo que producirá que los errores estándar de los coeficientes estén sesgados. Esto conduce a errores en tests de inferencia, como (por ejemplo) al determinar si un coeficiente es significativo.


\subsubsection{Test de Breusch-Pagan}

El test de Breusch-Pagan es uno de los más clásicos para testear homocedasticidad. El estadístico se distribuye $n\chi^2$ con $k$ grados de libertad. La hipótesis nula corresponde a un modelo homocedástico, de forma que si el estadístico tiene un $p$-value menor a un límite razonable (ej. $p<0.05$) entonces rechazamos la nula y decimos que el modelo es heterocedástico.

Podemos usar \verb|car::ncvTest()| para realizar el test de Breusch-Pagan original:

<<>>=
library(car)
ncvTest(modelo)
@

Sabemos que \verb|modelo| es heterocedástico (así lo construimos), y esto se confirma con un valor de $p$ muy cercano a 0: tiene 15 ceros a la derecha del separador de decimales!.

También es posible calcular la versión ``estudientizada'' del test, propuesta por \textcite{koenker_note_1981}. Esta versión más robusta que la original\footnote{Más detalles de las diferencias en \url{https://stats.stackexchange.com/a/193070/91358} y en \textcite{koenker_note_1981}.}, y puede ser calculada con \verb|lmtest::bptest()|:

<<>>=
library(lmtest)
bptest(modelo)
@


\subsubsection{Matriz de covarianza robusta}
\label{sec:matriz_cov_robusta}

Como mencioné, en presencia de heterocedasticidad los coeficientes estimados por MCO se mantienen insesgados, pero su varianza estimada será incorrecta. Para calcular una matriz de covarianzas robusta a heterocedasticidad usamos la función \verb|cars::hccm()| o \verb|sandwich::vcovHC()|.

Por ejemplo, continuando con nuestro modelo guardado en \verb|mincer|, podemos verificar que presenta heterocedasticidad y luego calcular la matriz de covarianzas corregida:

<<>>=
# Testear existencia de heterocedasticidad:
ncvTest(mincer)
# Calcular matriz de covarianzas robusta:
hccm(mincer)
@

\verb|hccm()| aplica la llamada ``corrección de White'' para el cálculo de la matriz. 
Es interesante verificar empíricamente una consecuencia importante a la hora de usar esta corrección: los errores robustos serán insesgados (que es lo que queremos), pero el costo es pérdida de eficiencia, es decir, errores más grandes. Compara el resultado anterior con la matriz que calculamos al usar \verb|vcov(mincer)|.

También podemos usar \verb|vcovHC()| para calcular la matriz. Esta función toma como método ``base'' la corrección de White, pero tiene disponibles otros métodos que son variaciones esa base. El método por defecto es HC3, que es equivalente a la opción \verb|robust| del comando \verb|regress| de Stata.\footnote{Ver \url{https://stats.stackexchange.com/q/117052/91358} para una explicación breve sobre replicar el método de Stata. \textcite{long_using_2000} conducen varias simulaciones para estudiar los distintos estimadores HC implementados en \texttt{vcovHC()}.}

Si no nos interesa la matriz de covarianzas en sí misma y sólo queremos obtener nuestros resultados con errores estándar robustos, podemos usar \verb|summary()| con la opción \verb|robust=TRUE|. Esta opción internamente usa \verb|vcovHC()| con HC3 (ver nota al pie):

<<eval=FALSE>>=
summary(mincer, robust = TRUE)
@

%todo: seccion de autocorrelacion

\subsection{Tests de hipótesis lineal}

El paquete \paq{car} incluye la función \verb|linearHypothesis()| para realizar tests de hipótesis lineal. Puede realizarse un test F o test de Wald, usando la matriz de covarianza regular o ajustada, dependiendo de nuestras especificaciones. Para realizar un test tenemos que construir una matriz de hipótesis y un vector de resultados. Por ejemplo, supongamos que tenemos un modelo lineal con 5 parámetros (incluyendo el intercepto):
\begin{equation*}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4
\end{equation*}
y queremos testear

\begin{equation*}
H_0 : \; \beta_0 = 0 \; \wedge \; \beta_2 + \beta_3 = 1.
\end{equation*}

En este caso la matriz de hipótesis y el vector de resultado serían

\begin{equation*}
\begin{bmatrix}
    1 & 0 & 0 & 0 & 0 \\
    0 & 0 & 1 & 1 & 0 \\
\end{bmatrix}
\beta = 
\begin{bmatrix}
    0 \\
    1 \\
\end{bmatrix}
\end{equation*}

En R implementamos esto de la siguiente forma:

<<eval=FALSE>>=
modelo <- lm(y ~ x1 + x2 + x3 + x4)
mh <- rbind(c(1,0,0,0,0), c(0,0,1,1,0))
vr <- c(0,1)
linearHypothesis(modelo, mh, vr)
@

Si \verb|modelo| es un objeto \verb|lm| (como en este caso), se realizará un test $F$ por defecto, mientras que si el objeto es \verb|glm| entonces se realizará un test de Wald $\chi^2$. El tipo de test puede cambiarse por medio de la opción \verb|type|. Asegúrate de revisar el documento de ayuda escribiendo \verb|?linearHypothesis| para ver más detalles.

Podemos realizar un test de hipótesis usando una matriz de covarianzas robusta. La manera más simple de aplicar la corrección de White es con el argumento \verb|white.adjust|:

<<eval=FALSE>>=
linearHypothesis(modelo, mh, vr, white.adjust = TRUE)
@

En términos más generales, podemos pasar a \verb|linearHypothesis()| la matriz de covarianzas que queramos (como las que calculamos en \ref{sec:matriz_cov_robusta}) por medio del argumento \verb|vcov|. Por ejemplo, podríamos pasar una matriz de covarianzas con la corrección de Newey-West al test de hipótesis lineal escribiendo

<<eval=FALSE>>=
linearHypothesis(modelo, mh, vr, vcov = NeweyWest(modelo))
@

Entonces \verb|white.adjust = TRUE| es equivalente a escribir \verb|vcov = hccm(modelo)|.


\subsection{Mínimos cuadrados ponderados}

Un modelo de mínimos cuadrados ponderados es una respuesta directa al problema de heterocedasticidad. En el modelo lineal simple asumimos que la variación del error es constante para todos los valores de las variables dependientes. Si este supuesto no se cumple, entonces hay puntos que en realidad entregan menos información sobre el proceso que estamos tratando de explicar, y parece razonable entonces penalizar estos puntos en la ponderación. Mínimos cuadrados ponderados es una implementación de esta idea, y permite maximizar la eficiencia de los parámetros estimados en presencia de heterocedasticidad.

Para realizar una estimación por mínimos cuadrados ponderados tenemos que pasar un vector con las ponderaciones al argumento \verb|weights| de \verb|lm()|:

<<>>=
modelo.mcp <- lm(smokes ~ 0 + male, data = smokerdata, weights = ponderaciones)
@


\subsection{Modelos con variables categóricas}

Una variable categórica es cualquiera donde los valores no tienen valor numérico en sí mismo, si no que representan grupos o categorías. Por ejemplo, una variable que indica la región de Chile en la que se ubica una empresa es categórica. Cada región tiene un número asignado, pero dicho número solo codifica un significado; no el valor en sí no debe usarse como un número.

En R las variables categóricas se denominan factores. Puedes chequear que una variable sea factor con la función \verb|is.factor()|:

<<>>=
is.factor(algo)
@

Si una variable es un factor, entonces al incluirla en la regresión se producirá automáticamente un conjunto de dummies por cada nivel del factor.

En caso de que una variable no esté guardada como factor, puedes usar \verb|factor()| para transformarla. Esto funciona dentro de la regresión también:

<<>>=
summary(lm(pc ~ emple + factor(region)))
@



\subsection{Errores no normales}

Por el Teorema de Gauss-Markov sabemos que un estimador MCO es el mejor estimador lineal insesgado, o MELI. En este contexto ``mejor'' quiere decir que es óptimo en térmminos de minimizar el error cuadrático medio. Esto ocurre siempre que los errores
\begin{itemize}
\item tengan promedio igual a cero
\item no estén correlacionados
\item tengan varianza constante.
\end{itemize}

Notar que no se exige normalidad de los errores, y ni siquiera es necesario que sean independientes e idénticamente distribuidos (IID).

Sin embargo, la normalidad de los errores es importante para hacer inferencia sobre los estimadores, y esto es clave para nosotros, porque permite evaluar si es que un coeficiente estimado es estadísticamente distinto de cero.

\section{Datos de panel}

Una base de datos de corte transversal (como las que hemos visto hasta ahora) posee una estructura interna unidimensional. Cada unidad (persona, empresa, familia) está indexada por $i$, y la base de datos es una colección de observaciones independientes.
En un panel los datos poseen una estructura interna indexada por un arreglo de dos dimensiones: una dimensión transversal de unidades indexadas por $i$, y una dimensión temporal indexada por $t$. En términos intuitivos, un panel es equivalente a tener varias bases de corte transversal donde observamos a las mismas unidades en distintos períodos.

\begin{table}[h]
\centering
\caption{Ejemplo simple de la estructura de un panel}
\label{tab:panel_simple}
\ttfamily
\begin{tabular}{rrr}
  \toprule
  ID & Periodo & X \\
  \midrule
  101 & 2015 & 6.5 \\
  102 & 2015 & 5.2 \\
  103 & 2015 & 5.9 \\
  \midrule
  101 & 2016 & 6.7 \\
  102 & 2016 & 5.5 \\
  103 & 2016 & 6.0 \\
  \bottomrule
\end{tabular}
\end{table}

%todo: hacer dibujo de arreglo 3d

%todo: 2 tablas side by side mostrando estructuras transversal y panel

En estricto rigor la nueva dimensión del panel, indexada por $t$, no tiene que representar períodos. Por ejemplo, podríamos tener un panel donde observamos a los mismos estudiantes en distintas asignaturas, y de esa forma $t$ sería un índice de asignatura. Sin embargo, en la gran mayoría de los casos se asume implícitamente que la nueva dimensión del panel representa períodos de algún tipo.

En el contexto de econometría, los modelos de efecto fijo y de efecto aleatorio son modelos de datos de panel que toman en cuenta la variación transversal de los datos. Sea $i=1,\ldots,n$ el índice transversal y $t=1,\ldots,T$ el índice temporal (o más en general, el índice que varía dentro de un grupo). Definimos un modelo de efecto fijo como

\begin{equation}
y_{it} = \alpha + u_i + \beta X_{it} + \epsilon_{it}.
\label{eq:FE-RE}
\end{equation}

Podemos entender este modelo, por ejemplo, como un panel de 30 estudiantes que son observados en tres períodos: 4\textsuperscript{to} básico, 8\textsuperscript{vo} básico y 4\textsuperscript{to} medio. Cada estudiante está identificado por el subíndice $i=1,\ldots,30$ y cada período en el tiempo está identificado por el subíndice $t=1,2,3$. Los valores que toman los indicadores no son relevantes, de forma que $i$ podría haber sido una variable de ID de alumno y $t$ podría haber sido una variable del grado del estudiante.

En este modelo cada unidad identificada por $i$ tiene un intercepto que es invariante en el tiempo, $\alpha + u_i$. En términos del ejemplo, cada estudiante tiene un intercepto distinto. Es por esto que usualmente nos interesa solo incluir el efecto, pero los sigue interesando solamente $\beta$.

Un modelo de efecto aleatorio se define por la misma ecuación, pero se impone la restricción adicional de que el efecto específico a cada unidad no está correlacionado con las variables independientes $X_{it}$, es decir, $\mathrm{E}[u_i, X_{it}] = 0$. En términos intuitivos esto corresponde a una versión más estricta del efecto fijo, ya que este último permite correlación entre dicho efecto y las variables independientes. Es lamentable que el nombre de ``efecto fijo'' y ``efecto aleatorio'' tengan poca relación con lo que realmente significa cada uno.

\subsection{Efectos fijos}

La manera más directa de definir un modelo de efecto fijo es incluyendo una dummy por cada unidad, es decir, haciendo que el indicador transversal $i$ sea un factor.

<<>>=
lm(y ~ factor(index) + x)
@

Sin embargo esta solución puede resultar incómoda si el número de unidades es muy grande. Por ejemplo, si tenemos un panel de 3000 estudiantes entonces se calculará explícitamente dicha cantidad de coeficientes ($u_i$), los cuales no nos interesan.

Otra manera común de estimar un modelo de efecto fijo es quitando el efecto fijo por la vía de estimar un modelo de diferencia de medias:

\begin{equation}
(y_{it} - \overline{y}_i) = \alpha + \beta (X_{it} - \overline{X}_i)  + \zeta{it}.
\label{eq:within_estimator}
\end{equation}

Al estimar \eqref{eq:within_estimator} obtendremos el llamado \eng{within estimator}, que indica la variación promedio a lo largo de $t$ para cada grupo definido por $i$