\documentclass{article}

\usepackage{polyglossia}
  \setmainlanguage{spanish}
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{pgfplots}
  \pgfplotsset{compat=1.12}
  \makeatletter
    \pgfdeclareplotmark{dot}
      {%
        \fill circle [x radius=0.04, y radius=0.16];
      }%
  \makeatother
\usepackage{booktabs}
\usepackage[backend=bibtex,style=authoryear,citestyle=authoryear,doi=false,isbn=false,url=false]{biblatex}
  \addbibresource{NER-bib.bib}
\usepackage[hidelinks,spanish]{hyperref}

\newcommand*{\eng}[1]{\textsl{#1}}
\newcommand*{\paq}[1]{\textbf{#1}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}

% Eliminar numeración en títulos menores que secciones
\setcounter{secnumdepth}{2}

% Code chunk
<<setup, include=FALSE>>=
library(knitr)
library(highr)
knitr::opts_chunk$set(fig.path="figs/", fig.width=4.8, fig.height=3.6, out.width="4in", out.height="3in", fig.align='center', dev='pdf')
@


\title{Notas de Econometría en R}
\author{Alvaro Carril\thanks{Estas notas son un intento de condensar lo justo y necesario para poder seguir un curso de econometría con R.
Luego de recopilar material de muchas clases y ayudantías de econometría, en general pensadas para ser aplicadas en Stata, decidí que podía intentar hacer estas clases en R y usar esta guía como material de apoyo.
Mi prioridad fue ser breve y conciso, por lo que estas notas no son un buen comienzo para aprender R (o econometría); son un complemento.
Si hay algún error o quieres hacer algún comentario, mi correo es \texttt{acarril@fen.uchile.cl}.}}



\begin{document}

\maketitle
\tableofcontents

\clearpage

\section{Introducción}

En las últimas décadas la economía se ha ido apropiando de los computadores para realizar trabajo empírico y, sin embargo, el enfoque de los libros de econometría sigue siendo eminentemente teórico ---no \emph{aplicado}.

A mi parecer, existen actualmente dos maneras de abordar la enseñanza de econometría. La primera es la clásica, o \emph{matemática}: derivando los modelos desde como ejercicios de álgebra lineal, y en general priorizando aspectos técnicos por sobre los conceptuales o aplicados. Vemos esto en libros que discuten en extensión los supuestos detrás de las formas funcionales o supuestos de distribución, pero que dedican pocas palabras a técnicas aplicadas como métodos de regresión discontinua.

Por otro lado hay quienes, de la mano con el crecimiento de la economía aplicada, abogan por una enseñanza aplicada de econometría, con énfasis en técnicas de 

\subsection{Qué es R}

R es un ambiente de software y un lenguaje de programación interpretado para hacer análisis estadístico y gráficos. Es una implementación de S, un lenguaje de programación matemático orientado a objetos más antiguo. Es software libre y de código abierto, activamente usado y ampliado por estadísticos y colaboradores de otras disciplinas.

R es mucho más flexible que la mayoría de los paquetes estadísticos normalmente usados por economistas. Es un lenguaje de programación completamente desarrollado, no sólo un programa con tests y métodos pre-programados.


\subsection{Obtener R}

La instalación base de R puede obtenerse de \url{https://www.r-project.org/}. Una vez instalado puedes usar RGui para usarlo. Sin embargo, usualmente yo tomo dos pasos adicionales para obtener más flexibilidad. El primero es instalar RStudio, un IDE para R que incluye una consola, editor con resaltado de sintaxis, historial de comandos y varias otras vainas que lo hacen más útil que RGui, especialmente si eres principiante. Puedes descargar RStudio de \url{https://www.rstudio.com/}, y también es gratis.

Finalmente, a mi me gusta poder interactuar con R desde la línea de comandos, igual que puedo hacer con Python o Julia. Para poder hacer esto en Windows tenemos que ejecutar R:

<<eval=FALSE>>=
"C:\Program Files\R\R-3.4.0\bin\x64\R"
@

Obviamente, tienes que reemplazar este directorio por el que corresponde a tu instalación. 

\includegraphics[width=\linewidth]{sss/r-win-command-prompt}

\subsection{Usar R interactivamente y a través de scripts}

<<>>=
getwd()
@



\section{Trabajando con datos}

\subsection{Objetos y asignaciones}

R es un lenguaje interpretado, lo que significa que ejecuta nuestras instrucciones directamente, sin compilar un programa previo. Podemos usar R interactivamente a través de la consola:
<<>>=
1+2
@

La mayoría de las operaciones y funciones en R no guardan el resultado de su ejecución. Por ejemplo, el resultado anterior (\verb|3|) es calculado pero no puede ser reutilizado sin volverse a calcular. Para lograr esto tenemos que asignar el resultado de la operación a un objeto:
<<>>=
x <- 1+2
@



\section{Regresión lineal}

<<echo=FALSE>>=
rm(list=ls())
@

El modelo de regresión lineal es el caballo de batalla de la econometría aplicada.
En su forma más simple usualmente se escribe como
\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \mu_i.
\label{eq:modelo_lineal_simple}
\end{equation}
donde $i = 1, \ldots, n$ es un índice que identifica a las observaciones (filas) en los datos. Podríamos tener $n$ personas, familias o empresas. Los términos $x$ e $y$ son vectores de datos de tamaño $n$.

En este modelo suponemos que la variable dependiente $y$ es una función lineal de la variable independiente, $x$. Entonces intentaremos buscar ---o \emph{estimar}--- una combinación de parámetros $\beta_0$ y $\beta_1$ que se ajuste a nuestros datos. Como es posible que los datos no tengan una relación lineal perfecta, el modelo incluye un término de error $\mu_i$. Este término representa otras variables que afectan a $y_i$ que no sean $x_i$.

Por ejemplo, podríamos tener un vector $y$ con los salarios de $n$ personas y modelarlo como una función lineal de los años de educación de esas personas, $x$. Resulta útil tener una base de datos concreta con estos vectores, por lo que usaremos 

<<results=TRUE>>=
simdatos <- read.csv(
  "https://raw.githubusercontent.com/acarril/NER/master/datos/simdatos.csv")
head(simdatos)
@

El comando \verb|read.csv()| permite leer bases de datos en formato CSV. Lo usamos para leer una base en línea y luego asignamos esos datos al objeto \verb|simdatos| (datos simulados). 
La función \verb|head()| sirve para imprimir el encabezado de los datos, lo que permite hacernos una idea rápida de las variables y sus valores.
Esta base tiene 30 observaciones (puedes imprimirla completa ejecutado \verb|simdatos|). La variable \verb|y| representa ingreso por hora (en miles de pesos) y la variable \verb|x| representa años de educación.

Siempre es recomendable graficar los datos, ya que un gráfico entrega mucha información y revela patrones. Para esto usaremos \paq{ggplot2}, un paquete muy poderoso para crear gráficos de distintos tipos. Lo cargamos y luego creamos un gráfico de puntos con los datos:

<<>>=
library(ggplot2)
ggplot(simdatos, aes(x, y)) + geom_point()
@

Vemos que existe una evidente relación lineal entre $x$ e $y$: a mayor nivel de educación parece haber mayor nivel de ingreso.
Esto es bueno para nosotros, ya que queremos ajustar los datos a un modelo lineal como el que escribimos en \eqref{eq:modelo_lineal_simple}.
Cabe preguntarse ahora cuál es la manera óptima de elegir los parámetros que determinan dicha relación lineal, es decir, ¿cómo podemos elegir $\beta_0$ (el intercepto) y $\beta_1$ (la pendiente) para que la predicción se ajuste lo mejor posible a los datos?

Podríamos elegir entre una infinita variedad de combinaciones de $\beta_0$ y $\beta_1$ para modelar los datos. Por ejemplo, el código de abajo simula 150 líneas con interceptos y pendientes ``razonables'':

<<>>=
set.seed(314)
modelos <- data.frame(
  beta1 = runif(150, -3, 1),
  beta2 = runif(150, -1, 1)
)
ggplot(simdatos, aes(x, y)) +
  geom_point() +
  geom_abline(
    aes(intercept = beta1, slope = beta2),
    data = modelos, alpha = 1/4
  )
@

Si bien hay muchas líneas que claramente no se ajustan bien a los datos, varias otras sí lo hacen, y resulta difícil determinar cuál es ``la mejor'' (asumiendo que tal línea existe).
Necesitamos un criterio riguroso para elegir los parámetros $\beta_0$ y $\beta_1$ de manera óptima, y esto es lo que haremos a continuación.


\subsection{Mínimos Cuadrados Ordinarios}
\label{sec:MCO}

El método de Mínimos Cuadrados Ordinarios (MCO) es una respuesta directa a la pregunta que planteamos recién: ¿qué criterio conviene usar para elegir el modelo lineal que se ajuste ``mejor'' a los datos?
Geométricamente, el método de MCO equivale a minimizar la distancia vertical entre los puntos y la recta definida por el modelo.

<<echo=FALSE, message=FALSE>>=
library(dplyr)
dist1 <- simdatos %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge,
    pred = -1 + x1 * 0.4
  )

ggplot(dist1, aes(x1, y)) + 
  geom_abline(intercept = -1, slope = 0.4, colour = "blue") +
  geom_linerange(aes(ymin = y, ymax = pred), colour = "red") +
  geom_point(colour = "grey40")
@

Recordemos que nuestro modelo original es
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i + \mu_i.
\end{equation*}
Dada una muestra de datos (como \verb|simdatos|), llamamos $\hat\beta_0$ y $\hat\beta_1$ a los candidatos para los coeficientes de la estimación. Entonces la recta $y_i$ que estimamos está definida como
\begin{equation}
\hat y_i = \hat\beta_0 + \hat\beta_1 x_i.
\end{equation}

La diferencia entre $y_i$ (los datos) y $\hat y_i$ (nuestra predicción) corresponde a $\hat\mu_i$, el residuo de nuestra estimación para cada observación $i$: 
\begin{align}
\hat \mu_i &= y_i - \hat y_i \notag \\
           &= y_i - \hat\beta_0 + \hat\beta_1 x_i.
\label{eq:residuos}
\end{align}
Gráficamente, la distancia entre cada punto (los datos) y la recta (la predicción $\hat y_i$)  corresponde al residuo, $\hat\mu_i$, representada en el gráfico por las líneas rojas. 
Es decir que para cada observación $i$ tenemos una medida de la magnitud del error que esta elección de $\hat\beta_0$ y $\hat\beta_1$ produce.
El método de MCO corresponde a elegir $\hat\beta_0$ y $\hat\beta_1$ en \eqref{eq:residuos} tal que se minimice la suma de los residuos al cuadrado:\footnote{¿Por qué al cuadrado? Porque se asume que es irrelevante si el error es una subestimación o una sobreestimación, de forma que lo único que importa es la magnitud. ¿Por qué no se usa el valor absoluto entonces? Buena pregunta: \url{https://stats.stackexchange.com/q/46019/91358}. Básicamente, porque es algebraicamente más simple manejar valores al cuadrado, y no hace ninguna diferencia en el resultado final.}

\begin{align}
\min_{\hat\beta_0,\hat\beta_1} & \sum_{i=1}^n \hat\mu_i^2 \notag \\
\Leftrightarrow \min_{\hat\beta_0,\hat\beta_1} & \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1 x_i)^2.
\end{align}

Al resolver este problema de minimización obtendremos los estimadores MCO:\footnote{La derivación paso a paso puede encontrarse, por ejemplo, en \textcite[cap. 2]{wooldridge_introductory_2013}.}

\begin{align}
\hat \beta_1 &= \sum_{i=1}^n \frac{\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n (x_i - \bar x)^2} \notag \\
 &= \frac{\Cov (x,y)}{\Var(x)} \\
\hat \beta_0 &= \bar y - \hat\beta_1 \bar x
\end{align}
donde $\bar x$ e $\bar y$ son los promedios de $x_i$ e $y_i$, $\Var(x)$ es la varianza muestral de $x$ y $\Cov(x,y)$ es la covarianza muestral de $x$ e $y$.
Todo esto significa que para encontrar los estimadores MCO y obtener la línea de mejor ajuste simplemente tenemos que calcular cuatro cosas: $\bar y$, $\bar x$, $\Cov (x,y)$ y $\Var (x,y)$. ¡Hagámoslo ahora!

<<>>=
# Indicar base de datos a usar
attach(simdatos)
# Promedios de 'x' e 'y'
x.barra <- mean(x)
y.barra <- mean(y)
# Varianza de 'x' y covarianza entre 'x' e 'y'
var.x <- var(x)
cov.xy <- cov(x,y)
# Calcular parámetros MCO
beta1 <- cov.xy/var.x
beta0 <- y.barra - beta1*x.barra
# 'Desactivar' base de datos
detach(simdatos)
# Imprimir parámetros calculados
beta0
beta1
@
Vemos que es fácil calcular los estimadores MCO y en este caso $\hat \beta_0 = -1.19$ y $\hat \beta_1 = 0.43$.

Ahora podemos usar los parámetros estimados para graficar $\hat y_i$ sobre los datos:

<<>>=
# Graficar línea de regresión MCO
ggplot(simdatos, aes(x,y)) + 
  expand_limits(x = 0, y = -2.5) +
  geom_point() +
  geom_abline(
    intercept = beta0, slope = beta1, 
    color = "blue", show.legend = FALSE )
@

Si bien el método que acabamos de usar para calcular $\hat\beta_0$ y $\hat\beta_1$ es útil para entender de manera más profunda qué es lo que estamos haciendo al estimar un modelo líneal por el método de MCO, no es demasiado breve. Podemos obtener los parámetros directamente usando la función \verb|lm()| de la siguiente forma:

<<>>=
lm(y ~ x, data = simdatos)
@

La función \verb|lm()| viene de \eng{linear model}, y permite calcular rápidamente los parámetros de un modelo lineal (¡usamos una línea!). El primer argumento de la función es una fórmula de R, que no es exactamente lo mismo que una fórmula en el sentido usual de la palabra. Por el momento basta entender que a la izquierda de \verb|~| indicamos la variable dependiente, mientras que a la derecha indicamos las variables independientes. Entonces \verb|lm()| toma una fórmula como \verb|y~x| y la traduce automáticamente a algo como \verb|y = beta0 + beta1 * x|.

Una de las ventajas de usar \verb|lm()| es que podemos guardar sus resultados en un objeto, el que luego podemos manipular para extraer otro tipo de información útil del modelo. A continuación veremos cómo extraer, manipular e interpretar esta información, lo que nos dará una comprensión mucho más profunda de lo que estamos haciendo.


\subsection{Interpretando un modelo lineal}

Usaremos ahora la base \verb|wage2|, que es uno de los datos usados por \textcite{wooldridge_introductory_2013} y contiene información de ingresos (y otras variables) para 935 personas. \footnote{Esta base es un subconjunto de los datos del estudio de \textcite{blackburn_unobserved_1992}.}

<<>>=
library(foreign)
ingresos <- read.dta("http://fmwww.bc.edu/ec-p/data/wooldridge/wage2.dta")
ingresos <- as.data.frame(ingresos[c("wage","educ")])
@

El paquete \paq{foreign} nos permite leer bases de datos en formatos de otros programas. En este caso usamos la función \verb|foreign::read.dta()| para leer una base de datos de Stata.
Por el momento nos interesan dos variables: \verb|wage|, que es un vector de ingresos (en dólares menusales) y \verb|educ|, que es un vector con años de escolaridad de cada persona. La última línea guarda solamente esas dos variables, como marco de datos, en el objeto \verb|ingresos|.

Al cargar nuevos datos yo siempre recomiendo:
\begin{enumerate}
\item Imprimir el encabezado
\item Imprimir un resumen
\item Graficarlos
\end{enumerate}

Escribimos lo siguiente para lograr estas tres cosas:

<<>>=
head(ingresos)
summary(ingresos)
ggplot(ingresos, aes(educ, wage)) + geom_point()
@

Es evidente que estos datos (reales) no presentan una relación lineal tan evidente. Más adelante nos haremos cargo de este problema.

\subsubsection{Coeficientes}

Volvemos a usar \verb|lm()| como ya lo hemos hecho, esta vez definiendo una fórmula donde \verb|wage| es la variable dependiente y \verb|educ| es la variable independiente:
<<>>=
lm(wage ~ educ, data = ingresos)
@
<<echo=FALSE, cache=FALSE>>=
options(digits = 2)
modelo <- lm(wage ~ educ, data = ingresos)
@

Los coeficientes estimados de un modelo lineal como este pueden interpretarse fácilmente, recordando que nuestra predicción es
\begin{equation*}
\hat y = \hat \beta_0 + \hat \beta_1 x.
\end{equation*}
Notar que esta ecuación es equivalente a una función lineal genérica, del tipo $y=a+bx$. Tener este simple hecho en cuenta es de gran ayuda para entender su interpretación.

El intercepto de un modelo lineal nos indica que si las variables independientes son todas iguales a 0, el modelo predice, en promedio, un valor de $\hat \beta_0$ para $\hat y$. En este ejemplo, si una persona tiene 0 años de educación el modelo predice un ingreso mensual promedio de \Sexpr{coefficients(modelo)[1]} dólares.
Los coeficientes estimados para las variables independientes (solamente $\hat \beta_1$, en este caso) pueden interpretarse como el cambio promedio que un aumento de una unidad de $x$ tiene sobre $y$. Nuestra estimación indica que un año adicional de educación tiene un efecto promedio de aumentar en \Sexpr{coefficients(modelo)[2]} dólares el ingreso de las personas. Si este coeficiente fuera negativo diríamos que un cambio en $x$ está correlacionado a una reducción de $y$.


\subsubsection{Errores estándar de los coeficientes}

Los parámetros estimados ---los coeficientes de MCO, en este caso--- son estadísticos muestrales que usamos para hacer inferencias de los parámetros poblacionales. Es importante tener en cuenta que son estos parámetros poblacionales los que nos interesan en realidad, pero como no podemos observarlos directamente, debemos inferirlos por medio de una muestra finita.

Es evidente que si tomáramos una muestra distinta a la actual, lo más probable es que estimaríamos parámetros diferentes. Si tomaras datos de otras 935 personas (o cualquier número, en realidad) es muy posible que al aplicar el mismo modelo obtuviéramos valores de $\beta_0$ y $\beta_1$ distintos de \Sexpr{coefficients(modelo)[1]} y \Sexpr{coefficients(modelo)[2]}. Además es muy probable que nuevamente ninguno de los dos sea igual al verdadero parámetro poblacional, que es el que nos interesa. Si continuáramos este proceso de tomar diferentes muestras y estimar los mismos parámetros una y otra vez, veríamos que la frecuencia relativa de las estimaciones obtenidas sigue una distribución de probabilidad, y por el Teorema Central del Límite sabemos que es probable que esta distribución sea normal. Entonces es importante para nuestra estimación que cuantifiquemos la cantidad de incertidumbre en esta distribución poblacional desconocida. Ahí es donde entra el concepto de error estándar, que corresponde a nuestra estimación de la desviación estándar de la distribución de estos muestreos. En términos intuitivos, es una medida de la incertidumbre de $\hat\beta$.

Encerramos \verb|lm()| dentro de \verb|summary()| para obtener información más detallada de las estimaciones del modelo, incluyendo estimaciones para los errores estándar de los coeficientes:

<<>>=
summary(lm(wage ~ educ, data = ingresos))
@

En el ejemplo, ...

%todo: explicar mejor todos los summary de ols

<<>>=
ggplot(ingresos, aes(educ, wage)) + 
  geom_point() + 
  geom_smooth(method = "lm")
@


\subsection{Extraer estadísticos de una regresión}

Para guardar el modelo asignamos \verb|lm()| a un objeto:

<<>>=
modelo <- lm(wage ~ educ, data = ingresos)
@

El objeto \verb|modelo| no solo incluye los coeficientes estimados, si no que una serie de otros elementos que son muy útiles ---veremos ahora por qué. En estricto rigor \verb|modelo| es lo que R llama una lista, y contiene una serie de otros objetos. Podemos imprimir los nombres de estos objetos escribiendo
<<>>=
names(modelo)
@

Para acceder a los objetos guardados usamos la notación de \verb|$|, como es usual en R. Por ejemplo, para imprimir los coeficientes del modelo escribimos
<<>>=
modelo$coefficients
@
También existen funciones para acceder directamente a los objetos. Por ejemplo,
<<>>=
coefficients(modelo)
@
En cualquier caso, el resultado es un vector cuyos elementos tienen nombres: el intercepto ($\beta_0$) es \texttt{(Intercept)} y el nombre del primer coeficiente ($\beta_1$) es el nombre de la variable $x$, es decir, \texttt{educ}. Usando esto podemos acceder a elementos específicos del vector usando sus nombres o sus índices, como se mencionó en [ref]:
<<>>=
coef(modelo)[1]
coef(modelo)["educ"]
@

\subsection{Predicciones y residuos}

La predicción de un modelo, a veces llamada valores ajustados, es simplemente el vector $\hat y_i$. Teniendo los valores ajustados podemos calcular también los residuos del modelo, $\mu_i$:
\begin{align}
\hat y_i &= \hat \beta_0 + \hat\beta_1 x_i \label{eq:prediccion} \\
\hat \mu_i &= y_i - \hat y_i \label{eq:residuos}
\end{align}
Podemos calcular fácilmente ambos vectores con los elementos que tenemos:
<<>>=
ingresos$prediccion <- coef(modelo)[1] + coef(modelo)[2] * ingresos$educ
ingresos$residuos <- ingresos$wage - ingresos$prediccion
@

Alternativamente, podemos calcular los valores predichos y los residuos del modelo usando funciones específicas para ello:
<<>>=
modelo.sim <- lm(y ~ x, data = simdatos)
simdatos$prediccion <- fitted(modelo.sim)
simdatos$residuos <- resid(modelo.sim)
@

Podemos graficar nuestra predicción sobre los datos para inspeccionar visualmente cómo se ve. Para esto volvemos a usar \paq{ggplot2}:
<<>>=
ggplot(ingresos, aes(educ, wage)) +
  geom_point() +
  geom_line(aes(y = prediccion), color = "blue")
@

También resulta interesante analizar los residuos, que son ``la otra cara de la moneda''. Si bien los valores predichos nos muestran el patrón que el modelo ha sido capaz de capturar, los residuos nos mostrarán lo que el modelo ha omitido. Recordar que los residuos son simplemente la diferencia entre el valor predicho y el real, como se indica en \eqref{eq:residuos}. Graficamos los residuos escribiendo

<<>>=
ggplot(ingresos, aes(educ, residuos)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_point()
@

<<>>=
ggplot(simdatos, aes(x, residuos)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_point()
@

Además de estos elementos directamente accesibles guardados en \verb|modelo|, existen una serie de funciones que podemos aplicar al objeto para realizar otros cálculos o resumir información útil. Por ejemplo, es muy común usar \verb|summary()| para obtener un resumen de información importante del modelo estimado:
<<>>=
summary(modelo)
@



\hrulefill

El modelo de regresión lineal es el caballo de batalla de la econometría aplicada, y puede escribirse como
\begin{equation}
y_i = x_i^T \beta + \epsilon_i,
\end{equation}
donde $i = 1, \ldots, n$ es un índice que identifica a las observaciones (filas) en los datos. Podríamos tener $n$ personas, familias o empresas.
También es común escribir el mismo modelo en forma matricial:
\begin{equation}
y = X \beta + \epsilon.
\end{equation}

En estas ecuaciones $y$ (o $y_i$) es un vector $n \times 1$ que representa la variable dependiente del modelo, es decir, la variable que queremos explicar. Por ejemplo, $y$ podría ser un vector de salario cuyos valores son los ingresos mensuales de $n$ personas.
La variable dependiente es explicada por un conjunto de $k$ variables independientes, usalmente denotadas por $X$. Entonces $X$ es una matriz $n\times k$, ya que contiene información de $k$ variables (columnas) para $n$ observaciones (filas).

Al estimar este tipo de modelos usualmente nos interesa determinar $\beta$, que representa la relación lineal entre $y$ y $X$, es decir, entre la variable dependiente y el conjunto de variables independientes que la explican. El vector $\beta$ contiene $k \times 1$ coeficientes de regresión, de forma que estimaremos un coeficiente por cada variable incluída en $X$.
Finalmente $\epsilon$ es el llamado término de error, un vector $n \times 1$ que captura las discrepancias para cada observación $i$ que resulta de imponer una relación lineal entre $X$ e $y$.

Para entender bien todo esto resulta muy útil manipular un modelo simple. Para esto usaremos \verb|sim1|, una base de datos simulada que contiene dos variables: \verb|x| e \verb|y|. Cargamos el paquete \paq{modelr} para poder acceder esta base de datos, y además cargamos \paq{ggplot2} para crear gráficos:

<<>>=
library(modelr)
library(ggplot2)
@

Primero que nada conviene imprimir el encabezado de la base de datos, que corresponde a las primeras observaciones.

<<eval=TRUE>>=
sim1
@

Esto es lo primero que debieses hacer antes de empezar a utilizar datos nuevos, ya que permite tener una idea rápida sobre ellos. Vemos que hay 30 observaciones ($n=30$) y que \verb|x| solo toma valores enteros (y hay valores repetidos), mientras que \verb|y| es continua.

Ahora graficamos estos datos para hacernos una idea de la relación que existe entre ambos:

<<>>=
ggplot(sim1, aes(x, y)) + geom_point()
@

Oh sorpresa, los datos simulados presentan una clara correlación positiva. Parece casi diseñado para ser modelado linealmente...
%La pregunta relevante, sin embargo, es cuál línea usar.
Recordemos que en este caso nuestro modelo lineal puede escribirse simplemente como

\begin{equation*}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i 
\end{equation*}

Entonces la pregunta relevante es ``¿qué línea es la que mejor captura la relación entre $x$ e $y$?''.
Notemos que en este caso bidimensional simple $\beta_0$ y $\beta_1$ representan el intercepto y la pendiente de la recta, respectivamente.
Por lo tanto la pregunta también puede plantearse como: ¿cuáles son los valores de $\beta_0$ y $\beta_1$ que describen mejor esta relación?


<<echo=FALSE>>=
models <- data.frame(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)
ggplot(sim1, aes(x, y)) +
  geom_point() +
  geom_abline(
    aes(intercept = a1, slope = a2),
    data = models, alpha = 1/4
  )
@

\subsection{Mínimos Cuadrados Ordinarios}

El método de Mínimos Cuadrados Ordinarios (MCO) es una respuesta directa a la pregunta que planteamos recién: ¿qué criterio conviene usar para elegir el modelo lineal que se ajuste ``mejor'' a los datos?
Lo que propone MCO es minimizar la suma de los errores al cuadrado.
Geométricamente, esto equivale a minimizar la distancia vertical entre los puntos y la recta definida por el modelo.

<<echo=FALSE, message=FALSE>>=
library(dplyr)
dist1 <- sim1 %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge,
    pred = 7 + x1 * 1.5
  )

ggplot(dist1, aes(x1, y)) + 
  geom_abline(intercept = 7, slope = 1.5, colour = "grey40") +
  geom_point(colour = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), colour = "blue")
@

La distancia entre un puntos y la recta el error de predicción del modelo, capturado por $\epsilon_i$. Es decir que para cada observación $i$ tenemos una medida de la magnitud del error que esta elección de $\beta_0$ y $\beta_1$ produce. El criterio de MCO entonces es elegir $\beta_0$ y $\beta_1$ tal que se minimice el promedio de todos los errores al cuadrado.\footnote{¿Por qué al cuadrado? Porque se asume que es irrelevante si el error es una subestimación o una sobreestimación, de forma que lo único que importa es la magnitud. ¿Por qué no se usa el valor absoluto entonces? Buena pregunta.}



\hrulefill
%Para esta sección usaremos el paquete \paq{AER}, que incluye varias bases de datos para econometría. Ahora usaremos \verb|AER::CPS1988|, que contiene información de salarios de EE.UU. en Marzo de 1988.

<<message=FALSE>>=
library(AER)
data("CPS1988")
@

Consideremos la vieja y confiable ecuación de salarios de Mincer:

\begin{equation}
\text{salario}_i = \beta_0 + \beta_1 \text{educación}_i + \beta_2 \text{experiencia}_i + \epsilon_i
\label{eq:mincer}
\end{equation}

En términos de la base \verb|CPS1988|, nos interesan por el momento las variables \verb|wage|, \verb|education| y \verb|experience|. Podemos estimar este modelo lineal simple escribiendo

<<>>=
lm(wage ~ education + experience, data = CPS1988)
@

El resultado que imprime \verb|lm()| es una versión muy abreviada de todo lo que fue calculado, ya que sólo entrega las estimaciones de los coeficientes $\beta$ del modelo. Para ver más detalles podemos guardar los resultados de \verb|lm()| en el objeto \verb|mincer| y luego aplicarle la función \verb|summary()|:

<<>>=
mincer <- lm(wage ~ education + experience, data = CPS1988)
summary(mincer)
@

Normalmente se usa la función \verb|summary()| en objetos de datos, tales comos vectores o marcos de datos, para obtener estadística descriptiva del objeto (promedio, desviación estándar, etc.). Al usarla en un objeto con resultados de una estimación ---como en este caso--- nos entregará información importantes de dicha estimación, como el $R^2$ del modelo o los errores estándar de los coeficientes.

Podemos extraer otro tipo de información y objetos del modelo. La función \verb|residuals()| entrega los residuos del modelo, y \verb|vcov()| entrega la matriz de varianzas-covarianzas de los coeficientes:

<<eval=FALSE>>=
# Residuos
residuals(mincer)
# Matriz de varianzas-covarianzas:
vcov(mincer)
@

Obviamente estas funciones sólo sirven para realizar otras computaciones y no para ser impresas directamente, y es por eso que he omitido su resultado.

Es común querer estimar un modelo sin intercepto, lo que se logra escribiendo el intercepto explícitamente en la fórmula, usualmente como 0:

<<>>=
lm(wage ~ 0 + education + experience, data = CPS1988)
@

Podemos especificar formas funcionales más complejas para nuestro modelo básico expresado en \eqref{eq:mincer}, incluyendo términos interactuados con \verb|*| y términos cuadráticos encerrados en la función \verb|I()|:

<<>>=
lm(wage ~ experience + I(experience^2) + education + education*parttime, 
   CPS1988)
@

También es posible aplicar a los vectores alguna transformación directamente, como el logaritmo o raíz cuadrada (con \verb|log()| y \verb|sqrt()|). Hay que ser cuidados con los valores que toman las variables a las que se aplican estas funciones, recordando que la raíz cuadrada no está definida para números negativos y que $\ln(0) \rightarrow -\infty$:

<<warning=FALSE>>=
sqrt(-1)
log(0)
@


\subsection{Extraer y calcular información de una regresión}

Un objeto de clase \verb|lm| ---como \verb|mincer|--- es una lista que contiene varios atributos. Estos atributos son objetos calculados por \verb|lm()| con información a la que podemos acceder. Para imprimir una lista completa de los atributos de un objeto usamos \verb|attributes()|:

<<>>=
attributes(mincer)
@

Para usar algún atributo necesitamos la notación de signo \verb|$|. Por ejemplo, podemos guardar el vector $\hat y$ con

<<>>=
yhat <- mincer$fitted.values
@

Esto puede ser útil, por ejemplo, al implementar el método de variables instrumentales en dos etapas, donde tenemos que rescatar los valores estimados $\hat x$ en la primera etapa para luego incluirlos en la ecuación original. Escribiendo \verb|?lm| puedes obtener más información sobre todos estos atributos.

Vimos que \verb|summary()| calcula e imprime otra información útil del resultado de una regresión. Análogo a \verb|lm()|, \verb|summary()| también tiene atributos a los que podemos acceder:

<<>>=
mincer.summ <- summary(mincer)
attributes(mincer.summ)
@

Usando estos atributos podemos calcular fácilmente algunos resultados útiles. Por ejemplo, podemos obtener la matriz de varianzas-covarianzas del modelo:

<<>>=
(mincer.summ$sigma)^2 * mincer.summ$cov.unscaled
@

Finalmente, además de los cálculos que podemos efectuar directamente con estos atributos, existen muchas funciones que toman un objeto de clase \verb|lm| y calculan directamente algún resultado. Por ejemplo, podríamos haber calculado la matriz de varianzas-covarianzas directamente con \verb|vcov()|:

<<>>=
vcov(mincer)
@

Otras funciones útiles para usar con este tipo de objetos son:

<<eval=FALSE>>=
# Calcular suma de los cuadrados de los residuos (SSR):
deviance(mincer)
# Extraer log-likelihood (LL):
logLik(mincer) # no tiene mucho sentido aquí porque no estimamos con MV
# Calcular el critero de información de Akaike (AIC):
AIC(mincer)
@

Notar que \verb|AIC()| define el criterio como
\begin{equation}
AIC = -2 \ln \widehat L(p) + K\cdot 2,
\end{equation}
donde $K$ es el número de parámetros estimados y $\widehat L(p)$ es la función de verosimilitud. En muchos casos se prefiere que el criterio sea $AIC/N$, es decir, usar el Criterio de Información Bayesiano (BIC). Esto es,
\begin{equation}
BIC = -2 \ln \widehat L(p) + K \cdot \ln N,
\end{equation}
donde $N$ es el número de observaciones.
Para calcular el BIC también usamos \verb|AIC()|, pero especificando un parámetro de penalización distinto:
<<>>=
AIC(mincer, k = log(NROW(CPS1988))) # fijar k=2 es igual que el AIC clásico
@


\subsection{Heterocedasticidad}

La heterocedasticidad es un problema que es más difícil de pronunciar que de entender: ocurre cuando la dispersión de las variables dependientes no es constante para distintos valores de la variable independiente. La \autoref{fig:heterocedasticidad} muestra datos heterocedásticos para el caso de una regresión bivariada, tanto para una variable dependiente discreta como continua.

\begin{figure}[htb]
\pgfmathsetseed{112}
\centering
\begin{subfigure}{.5\textwidth}
\makeatletter
        \pgfdeclareplotmark{dot}
        {%
            \fill circle [x radius=0.08, y radius=0.32];
        }%
\makeatother
  \centering
  \begin{tikzpicture}[ % Define Normal Probability Function
declare function={
            normal(\x,\m,\s) = 1/(2*\s*sqrt(pi))*exp(-(\x-\m)^2/(2*\s^2));
        },
    declare function={invgauss(\a,\b) = sqrt(-2*ln(\a))*cos(deg(2*pi*\b));}
       ]
\begin{axis}[
    %no markers,
    domain=0:12,
    zmin=0, zmax=1,
    xmin=0, xmax=3,
    samples=200,
   samples y=0,
    view={40}{30},
    axis lines=middle,
    enlarge y limits=false,
    xtick={0.5,1.5,2.5},
    xmajorgrids,
    xticklabels={},
    ytick=\empty,
%   xticklabels={$x_1$, $x_2$, $x_3$},
    ztick=\empty,
    xlabel=$x$, xlabel style={at={(rel axis cs:1,0,0)}, anchor=west},
    ylabel=$y$, ylabel style={at={(rel axis cs:0,1,0)}, anchor=south west},
    zlabel=$f(\mu)$, zlabel style={at={(rel axis cs:0,0,1)}, anchor=south},
    set layers, mark=cube
  ]

\pgfplotsinvokeforeach{0.5,1.5,2.5}{
\addplot3 [draw=none, fill=black, opacity=0.25, only marks, mark=dot, mark layer=like plot, samples=30, domain=0.1:2.9, on layer=axis background] (#1, {1.5*(#1-0.5)+3+invgauss(rnd,rnd)*#1}, 0);
}
\addplot3 [samples=2, samples y=0, domain=0:3] (x, {1.5*(x-0.5)+3}, 0);
\addplot3 [blue!50, thick] (0.5, x, {normal(x, 3, 0.5)});
\addplot3 [blue!50, thick] (1.5, x, {normal(x, 4.5, 1)});
\addplot3 [blue!50, thick] (2.5, x, {normal(x, 6, 1.5)});

\pgfplotsextra{
\begin{pgfonlayer}{axis background}
\draw [gray, on layer=axis background] (0.5, 3, 0) -- (0.5, 3, {normal(0,0,0.5)}) (0.5,0,0) -- (0.5,12,0)
    (1.5, 4.5, 0) -- (1.5, 4.5, {normal(0,0,1)}) (1.5,0,0) -- (1.5,12,0)
    (2.5, 6, 0) -- (2.5, 6, {normal(0,0,1.5)}) (2.5,0,0) -- (2.5,12,0);

\end{pgfonlayer}
}
\end{axis}

\end{tikzpicture}
  \caption{Variable dependiente discreta}
  \label{fig:heterocedasticidad_x_discreta}
\end{subfigure}%
\makeatletter
        \pgfdeclareplotmark{dot}
        {%
            \fill circle [x radius=0.02, y radius=0.08];
        }%
\makeatother
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tikzpicture}[ % Define Normal Probability Function
declare function={
            normal(\x,\m,\s) = 1/(2*\s*sqrt(pi))*exp(-(\x-\m)^2/(2*\s^2));
        },
    declare function={invgauss(\a,\b) = sqrt(-2*ln(\a))*cos(deg(2*pi*\b));}
       ]
\begin{axis}[
    %no markers,
    domain=0:12,
    zmin=0, zmax=1,
    xmin=0, xmax=3,
    samples=200,
   samples y=0,
    view={40}{30},
    axis lines=middle,
    enlarge y limits=false,
    xtick={0.5,1.5,2.5},
    xmajorgrids,
    xticklabels={},
    ytick=\empty,
%   xticklabels={$x_1$, $x_2$, $x_3$},
    ztick=\empty,
    xlabel=$x$, xlabel style={at={(rel axis cs:1,0,0)}, anchor=west},
    ylabel=$y$, ylabel style={at={(rel axis cs:0,1,0)}, anchor=south west},
    zlabel=$f(\mu)$, zlabel style={at={(rel axis cs:0,0,1)}, anchor=south},
    set layers, mark=cube
  ]

\addplot3 [gray!50, only marks, mark=dot, mark layer=like plot, samples=200, domain=0.1:2.9, on layer=axis background] (x, {1.5*(x-0.5)+3+invgauss(rnd,rnd)*x}, 0);
\addplot3 [samples=2, samples y=0, domain=0:3] (x, {1.5*(x-0.5)+3}, 0);
\addplot3 [blue!50, thick] (0.5, x, {normal(x, 3, 0.5)});
\addplot3 [blue!50, thick] (1.5, x, {normal(x, 4.5, 1)});
\addplot3 [blue!50, thick] (2.5, x, {normal(x, 6, 1.5)});

\pgfplotsextra{
\begin{pgfonlayer}{axis background}
\draw [gray, on layer=axis background] (0.5, 3, 0) -- (0.5, 3, {normal(0,0,0.5)}) (0.5,0,0) -- (0.5,12,0)
    (1.5, 4.5, 0) -- (1.5, 4.5, {normal(0,0,1)}) (1.5,0,0) -- (1.5,12,0)
    (2.5, 6, 0) -- (2.5, 6, {normal(0,0,1.5)}) (2.5,0,0) -- (2.5,12,0);

\end{pgfonlayer}
}
\end{axis}


\end{tikzpicture}
  \caption{Variable dependiente continua}
  \label{fig:heterocedasticidad_x_continua}
\end{subfigure}
\caption{Heterocedasticidad}
\label{fig:heterocedasticidad}
\end{figure}



Podemos generar fácilmente un conjunto de datos que presenten el problema:

<<fig.width=4.5, fig.height=4.5>>=
set.seed(314)
n <- 256                           # Número de observaciones
x <- (1:n)/n                       # Valores de `x`
e <- rnorm(n, sd=1)                # Valores aleatorios de una normal
i <- order(runif(n, max=dnorm(e))) # Ubicamos los más grandes al final
y <- 1 + 5 * x + e[rev(i)]         # Generamos `y` con el error `e`.
modelo <- lm(y ~ x)                # Guardamos el modelo lineal
plot(x, y)
abline(coef(modelo), col = "Red")
@

En presencia de heterocedasticidad una estimación por MCO seguirá entregando coeficientes consistentes e insesgados. Sin embargo, no podremos estimar correctamente la matriz de varianzas-covarianzas, lo que producirá que los errores estándar de los coeficientes estén sesgados. Esto conduce a errores en tests de inferencia, como (por ejemplo) al determinar si un coeficiente es significativo.


\subsubsection{Test de Breusch-Pagan}

El test de Breusch-Pagan es uno de los más clásicos para testear homocedasticidad. El estadístico se distribuye $n\chi^2$ con $k$ grados de libertad. La hipótesis nula corresponde a un modelo homocedástico, de forma que si el estadístico tiene un $p$-value menor a un límite razonable (ej. $p<0.05$) entonces rechazamos la nula y decimos que el modelo es heterocedástico.

Podemos usar \verb|car::ncvTest()| para realizar el test de Breusch-Pagan original:

<<>>=
library(car)
ncvTest(modelo)
@

Sabemos que \verb|modelo| es heterocedástico (así lo construimos), y esto se confirma con un valor de $p$ muy cercano a 0: tiene 15 ceros a la derecha del separador de decimales!.

También es posible calcular la versión ``estudientizada'' del test, propuesta por \textcite{koenker_note_1981}. Esta versión más robusta que la original\footnote{Más detalles de las diferencias en \url{https://stats.stackexchange.com/a/193070/91358} y en \textcite{koenker_note_1981}.}, y puede ser calculada con \verb|lmtest::bptest()|:

<<>>=
library(lmtest)
bptest(modelo)
@


\subsubsection{Matriz de covarianza robusta}
\label{sec:matriz_cov_robusta}

Como mencioné, en presencia de heterocedasticidad los coeficientes estimados por MCO se mantienen insesgados, pero su varianza estimada será incorrecta. Para calcular una matriz de covarianzas robusta a heterocedasticidad usamos la función \verb|cars::hccm()| o \verb|sandwich::vcovHC()|.

Por ejemplo, continuando con nuestro modelo guardado en \verb|mincer|, podemos verificar que presenta heterocedasticidad y luego calcular la matriz de covarianzas corregida:

<<>>=
# Testear existencia de heterocedasticidad:
ncvTest(mincer)
# Calcular matriz de covarianzas robusta:
hccm(mincer)
@

\verb|hccm()| aplica la llamada ``corrección de White'' para el cálculo de la matriz. 
Es interesante verificar empíricamente una consecuencia importante a la hora de usar esta corrección: los errores robustos serán insesgados (que es lo que queremos), pero el costo es pérdida de eficiencia, es decir, errores más grandes. Compara el resultado anterior con la matriz que calculamos al usar \verb|vcov(mincer)|.

También podemos usar \verb|vcovHC()| para calcular la matriz. Esta función toma como método ``base'' la corrección de White, pero tiene disponibles otros métodos que son variaciones esa base. El método por defecto es HC3, que es equivalente a la opción \verb|robust| del comando \verb|regress| de Stata.\footnote{Ver \url{https://stats.stackexchange.com/q/117052/91358} para una explicación breve sobre replicar el método de Stata. \textcite{long_using_2000} conducen varias simulaciones para estudiar los distintos estimadores HC implementados en \texttt{vcovHC()}.}

Si no nos interesa la matriz de covarianzas en sí misma y sólo queremos obtener nuestros resultados con errores estándar robustos, podemos usar \verb|summary()| con la opción \verb|robust=TRUE|. Esta opción internamente usa \verb|vcovHC()| con HC3 (ver nota al pie):

<<eval=FALSE>>=
summary(mincer, robust = TRUE)
@

%todo: seccion de autocorrelacion

\subsection{Tests de hipótesis lineal}

El paquete \paq{car} incluye la función \verb|linearHypothesis()| para realizar tests de hipótesis lineal. Puede realizarse un test F o test de Wald, usando la matriz de covarianza regular o ajustada, dependiendo de nuestras especificaciones. Para realizar un test tenemos que construir una matriz de hipótesis y un vector de resultados. Por ejemplo, supongamos que tenemos un modelo lineal con 5 parámetros (incluyendo el intercepto):
\begin{equation*}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4
\end{equation*}
y queremos testear

\begin{equation*}
H_0 : \; \beta_0 = 0 \; \wedge \; \beta_2 + \beta_3 = 1.
\end{equation*}

En este caso la matriz de hipótesis y el vector de resultado serían

\begin{equation*}
\begin{bmatrix}
    1 & 0 & 0 & 0 & 0 \\
    0 & 0 & 1 & 1 & 0 \\
\end{bmatrix}
\beta = 
\begin{bmatrix}
    0 \\
    1 \\
\end{bmatrix}
\end{equation*}

En R implementamos esto de la siguiente forma:

<<eval=FALSE>>=
modelo <- lm(y ~ x1 + x2 + x3 + x4)
mh <- rbind(c(1,0,0,0,0), c(0,0,1,1,0))
vr <- c(0,1)
linearHypothesis(modelo, mh, vr)
@

Si \verb|modelo| es un objeto \verb|lm| (como en este caso), se realizará un test $F$ por defecto, mientras que si el objeto es \verb|glm| entonces se realizará un test de Wald $\chi^2$. El tipo de test puede cambiarse por medio de la opción \verb|type|. Asegúrate de revisar el documento de ayuda escribiendo \verb|?linearHypothesis| para ver más detalles.

Podemos realizar un test de hipótesis usando una matriz de covarianzas robusta. La manera más simple de aplicar la corrección de White es con el argumento \verb|white.adjust|:

<<eval=FALSE>>=
linearHypothesis(modelo, mh, vr, white.adjust = TRUE)
@

En términos más generales, podemos pasar a \verb|linearHypothesis()| la matriz de covarianzas que queramos (como las que calculamos en \ref{sec:matriz_cov_robusta}) por medio del argumento \verb|vcov|. Por ejemplo, podríamos pasar una matriz de covarianzas con la corrección de Newey-West al test de hipótesis lineal escribiendo

<<eval=FALSE>>=
linearHypothesis(modelo, mh, vr, vcov = NeweyWest(modelo))
@

Entonces \verb|white.adjust = TRUE| es equivalente a escribir \verb|vcov = hccm(modelo)|.


\subsection{Mínimos cuadrados ponderados}

Un modelo de mínimos cuadrados ponderados es una respuesta directa al problema de heterocedasticidad. En el modelo lineal simple asumimos que la variación del error es constante para todos los valores de las variables dependientes. Si este supuesto no se cumple, entonces hay puntos que en realidad entregan menos información sobre el proceso que estamos tratando de explicar, y parece razonable entonces penalizar estos puntos en la ponderación. Mínimos cuadrados ponderados es una implementación de esta idea, y permite maximizar la eficiencia de los parámetros estimados en presencia de heterocedasticidad.

Para realizar una estimación por mínimos cuadrados ponderados tenemos que pasar un vector con las ponderaciones al argumento \verb|weights| de \verb|lm()|:

<<>>=
modelo.mcp <- lm(smokes ~ 0 + male, data = smokerdata, weights = ponderaciones)
@


\subsection{Modelos con variables categóricas}

Una variable categórica es cualquiera donde los valores no tienen valor numérico en sí mismo, si no que representan grupos o categorías. Por ejemplo, una variable que indica la región de Chile en la que se ubica una empresa es categórica. Cada región tiene un número asignado, pero dicho número solo codifica un significado; no el valor en sí no debe usarse como un número.

En R las variables categóricas se denominan factores. Puedes chequear que una variable sea factor con la función \verb|is.factor()|:

<<>>=
is.factor(algo)
@

Si una variable es un factor, entonces al incluirla en la regresión se producirá automáticamente un conjunto de dummies por cada nivel del factor.

En caso de que una variable no esté guardada como factor, puedes usar \verb|factor()| para transformarla. Esto funciona dentro de la regresión también:

<<>>=
summary(lm(pc ~ emple + factor(region)))
@


\section{Datos de panel}

Una base de datos de corte transversal (como las que hemos visto hasta ahora) posee una estructura interna unidimensional. Cada unidad (persona, empresa, familia) está indexada por $i$, y la base de datos es una colección de observaciones independientes.
En un panel los datos poseen una estructura interna indexada por un arreglo de dos dimensiones: una dimensión transversal de unidades indexadas por $i$, y una dimensión temporal indexada por $t$. En términos intuitivos, un panel es equivalente a tener varias bases de corte transversal donde observamos a las mismas unidades en distintos períodos.

\begin{table}[h]
\centering
\caption{Ejemplo simple de la estructura de un panel}
\label{tab:panel_simple}
\ttfamily
\begin{tabular}{rrr}
  \toprule
  ID & Periodo & X \\
  \midrule
  101 & 2015 & 6.5 \\
  102 & 2015 & 5.2 \\
  103 & 2015 & 5.9 \\
  \midrule
  101 & 2016 & 6.7 \\
  102 & 2016 & 5.5 \\
  103 & 2016 & 6.0 \\
  \bottomrule
\end{tabular}
\end{table}

%todo: hacer dibujo de arreglo 3d

%todo: 2 tablas side by side mostrando estructuras transversal y panel

En estricto rigor la nueva dimensión del panel, indexada por $t$, no tiene que representar períodos. Por ejemplo, podríamos tener un panel donde observamos a los mismos estudiantes en distintas asignaturas, y de esa forma $t$ sería un índice de asignatura. Sin embargo, en la gran mayoría de los casos se asume implícitamente que la nueva dimensión del panel representa períodos de algún tipo.

En el contexto de econometría, los modelos de efecto fijo y de efecto aleatorio son modelos de datos de panel que toman en cuenta la variación transversal de los datos. Sea $i=1,\ldots,n$ el índice transversal y $t=1,\ldots,T$ el índice temporal (o más en general, el índice que varía dentro de un grupo). Definimos un modelo de efecto fijo como

\begin{equation}
y_{it} = \alpha + u_i + \beta X_{it} + \epsilon_{it}.
\label{eq:FE-RE}
\end{equation}

Podemos entender este modelo, por ejemplo, como un panel de 30 estudiantes que son observados en tres períodos: 4\textsuperscript{to} básico, 8\textsuperscript{vo} básico y 4\textsuperscript{to} medio. Cada estudiante está identificado por el subíndice $i=1,\ldots,30$ y cada período en el tiempo está identificado por el subíndice $t=1,2,3$. Los valores que toman los indicadores no son relevantes, de forma que $i$ podría haber sido una variable de ID de alumno y $t$ podría haber sido una variable del grado del estudiante.

En este modelo cada unidad identificada por $i$ tiene un intercepto que es invariante en el tiempo, $\alpha + u_i$. En términos del ejemplo, cada estudiante tiene un intercepto distinto. Es por esto que usualmente nos interesa solo incluir el efecto, pero los sigue interesando solamente $\beta$.

Un modelo de efecto aleatorio se define por la misma ecuación, pero se impone la restricción adicional de que el efecto específico a cada unidad no está correlacionado con las variables independientes $X_{it}$, es decir, $\mathrm{E}[u_i, X_{it}] = 0$. En términos intuitivos esto corresponde a una versión más estricta del efecto fijo, ya que este último permite correlación entre dicho efecto y las variables independientes. Es lamentable que el nombre de ``efecto fijo'' y ``efecto aleatorio'' tengan poca relación con lo que realmente significa cada uno.

\subsection{Efectos fijos}

La manera más directa de definir un modelo de efecto fijo es incluyendo una dummy por cada unidad, es decir, haciendo que el indicador transversal $i$ sea un factor.

<<>>=
lm(y ~ factor(index) + x)
@

Sin embargo esta solución puede resultar incómoda si el número de unidades es muy grande. Por ejemplo, si tenemos un panel de 3000 estudiantes entonces se calculará explícitamente dicha cantidad de coeficientes ($u_i$), los cuales no nos interesan.

Otra manera común de estimar un modelo de efecto fijo es quitando el efecto fijo por la vía de estimar un modelo de diferencia de medias:

\begin{equation}
(y_{it} - \overline{y}_i) = \alpha + \beta (X_{it} - \overline{X}_i)  + \zeta{it}.
\label{eq:within_estimator}
\end{equation}

Al estimar \eqref{eq:within_estimator} obtendremos el llamado \eng{within estimator}, que indica la variación promedio a lo largo de $t$ para cada grupo definido por $i$
 
\nocite{*}
\printbibliography

\end{document}