<<cap_reglineal_simple, echo=FALSE>>=
set_parent('NER.Rnw')
@

\chapter{Regresión lineal simple}

<<echo=FALSE>>=
rm(list=ls())
@

El modelo de regresión lineal es el caballo de batalla de la econometría aplicada.
[Intro motivacional]

En este capítulo comenzaremos a usar R para cargar bases de datos, las que usaremos para estimar modelos lineales y crear gráficos. Estas herramientas nos ayudarán a ilustrar conceptos fundamentales del modelo de regresión lineal y los supuestos que lo sustentan. Nos apoyaremos de los siguientes paquetes, que deben ser cargados antes de correr los bloques que vendrán a continuación:

<<packages, message=FALSE>>=
library(tidyverse)
library(haven)
library(reshape2)
@

\begin{Rbox}
\verb|library()| es una función que nos permite cargar paquetes de R, los que contienen más funciones y datos. Usaremos \verb|library()| al comienzo de nuestro código, para mantener el orden. \paq{tidyverse} contiene una gran cantidad de paquetes (ver \autoref{sec:rbase_tidyverse}) y lo usaremos en todos los capítulos. \paq{haven} nos permite usar la función \verb|read_dta()| para leer una base de datos de Stata (con extensión \verb|*.dta|).
\end{Rbox}

Las preguntas que abordaremos en este capítulo involucran dos variables, que llamaremos $x$ e $y$.
Nos interesará entonces ``explicar $y$ en términos de $x$''.
Por ejemplo, $y$ podría ser la tasa de crímenes de varios barrios y $x$ el número de parques construidos en esos barrios.
O $y$ podría ser el porcentaje de los votos obtenidos por distintos candidatos y $x$ su gasto en la campaña electoral, etc.
Si bien esto parece muy simple (y de ahí el nombre del capítulo), los conceptos que veremos acá serán generalizados muy fácilmente a más variables.

Tomemos este último ejemplo y construyamos un caso concreto.
La \autoref{fig:votos_plot} muestra datos de 173 campañas entre los dos candidatos finalistas en elecciones de congreso estadounidenses.
Nos enfocamos en las variables de gasto y votos obtenidos por el ``candidato A'' (un nombre del que Kafka sin duda estaría orgulloso).
El eje $x$ corresponde al porcentaje gastado por este candidato (\verb|shareA|), mientras que el eje $y$ indica el porcentaje de los votos que obtuvo (\verb|voteA|).
El código para cargar los datos y crear el gráfico se muestra a continuación:

<<'votos_plot', fig.cap="Relación entre porcentaje de gasto (\\texttt{shareA}) y porcentaje de votos obtenidos (\\texttt{voteA})">>=
# Leer datos (Stata) y asignarlos al objetos 'votos'
votos <- read_dta("http://fmwww.bc.edu/ec-p/data/wooldridge/vote1.dta")
# Imprimir las primeras observaciones de las variables que nos interesan
select(votos, voteA, shareA)
# Graficar ambas variables
ggplot(votos, aes(shareA, voteA)) + geom_point()
@

\begin{Rbox}
Usando \verb|haven::read_dta()| podemos leer una base de datos de Stata (con extensión \verb|*.dta|), la que asignamos al objeto \verb|votos|. Luego usamos \verb|dplyr::select()| para seleccionar algunas variables e imprimirlas. Finalmente usamos las funciones \verb|ggplot2::ggplot()| y \verb|ggplot2::geom_point()| para crear un gráfico de puntos.
\end{Rbox}

Al intentar modelar la relación entre $x$ e $y$ surgen una serie de preguntas. Quizás la más inmediata es: ``¿cuál es la relación funcional entre ambas variables?''
Nuestro primer supuesto será que las variables se relacionan de manera lineal, es decir,
\begin{equation}
y = ax + b.
\label{eq:ec_lin}
\end{equation}

La ecuación \eqref{eq:ec_lin} corresponde a la clásica función lineal de colegio, y su interpretación geométrica es la misma: $a$ es un término que representa la pendiente de una recta, y $b$ es su intercepto.
Este supuesto de relación lineal parece cumplirse bastante bien para nuestro ejemplo de la \autoref{fig:votos_plot}: no sabemos qué valores deberían tomar $a$ y $b$, pero definitivamente pareciera existir alguna combinación de valores de $a$ y $b$ tal que una línea se ajuste ``bien'' a los datos.

Sin embargo, el modelo expresado en \eqref{eq:ec_lin} asume que existe una relación lineal \emph{perfecta} entre $x$ e $y$, lo que es altamente restrictivo. 
En términos de la \autoref{fig:votos_plot}, es como suponer que podríamos trazar una recta que pasara por \emph{todos} los puntos, lo que evidentemente es imposible.
Entonces surge naturalmente la necesidad de permitir que otros factores ---distintos de $x$--- afecten en cierta medida a $y$, de forma que nuestro modelo tenga un margen de error.
Esto se logra agregando un \kw{término de error}, denotado por $\mu$, al modelo lineal.

Adicionalmente, por un simple tema de convención desde ahora en adelante llamaremos $\beta_0$ al parámetro del intercepto y $\beta_1$ al parámetro de la pendiente.
Entonces podemos reescribir nuestro modelo, que ahora es
\begin{equation}
y = \beta_0 + \beta_1x + \mu.
\label{eq:modelo_lineal_simple_poblacional}
\end{equation}
Esta ecuación define el \kw{modelo de regresión lineal simple}. Entendamos bien qué quiere decir cada palabra de este título:

\begin{itemize}
\item \textbf{modelo}, porque es un intento de simplificar la realidad para enfocarnos en algunos aspectos que nos interesan
\item \textbf{de}, una preposición en la gramática del español
\item \textbf{regresión} es el término usado originalmente por \textcite{galton_regression_1886}, aunque conceptualmente no guarda mucha relación con lo que hacemos ahora\footnote{\textcite{stigler_history_1986} cuenta una historia más detallada al respecto.} (pero el nombre es \eng{cool})
\item \textbf{lineal}, porque estamos asumiendo que los aspectos de la realidad que nos interesan ---las variables $x$ e $y$--- se relacionan de manera lineal en parámetros
\item \textbf{simple}, porque estamos restringiendo el análisis a solamente dos variables (por el momento).
\end{itemize}

Muchos libros tienen nombres distintos para $x$ e $y$. Yo uso la convención (bastante establecida) de referirme a $y$ como la \kw{variable dependiente} del modelo, y a $x$ como la \kw{variable independiente} del modelo. Estas son las variables que ya tenemos, y que intentaremos relacionar.
Ya mencionamos que $\mu$ es el llamado \kw{término de error}, que captura factores que afectan a $y$ que no hemos incluido en el modelo. Ojo que $\mu$ no es igual a $u$, y se pronuncia <<mu>>. Esto es sólo para enredar, claro.

En conjunto nos referiremos a $\beta_0$ y $\beta_1$ como los \kw{parámetros} (o coeficientes) del modelo.
Estos son simples números, pero como no los tenemos, intentaremos \emph{estimarlos}.
Encontrar una forma razonable de estimar estos parámetros será el objetivo de este capítulo (¡y más allá!).
Emprendemos esta tarea a continuación.

\section{Estimando los parámetros del modelo}

Nuestro objetivo ahora es estimar los parámetros $\beta_0$ y $\beta_1$ del modelo de regresión simple explicitado en \eqref{eq:modelo_lineal_simple_poblacional}. En términos geométricos, queremos encontrar una forma de estimar el intercepto y la pendiente de una recta para que esta se ajuste ``bien'' a los datos.

El modelo expresado en \eqref{eq:modelo_lineal_simple_poblacional} es nuestro supuesto de cómo se relacionan variables \emph{poblacionales}.
Sin embargo, nosotros (con alta probabilidad) no tendremos acceso a datos de la población completa, si no que a una \emph{muestra}.
Entonces tenemos que adaptar nuestra ecuación para que represente un \kw{modelo muestral}.
Logramos esto simplemente suponiendo que contamos con una muestra aleatoria de la población completa. Llamamos ${(x_i, y_i): i = 1, \ldots, n}$ a una muestra aleatoria de tamaño $n$ de la población, es decir, $x_i$ e $y_i$ son vectores con $n$ observaciones. Entonces reescribimos el modelo en términos de esta muestra:
\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \mu_i,
\label{eq:modelo_lineal_simple}
\end{equation}
para todo $i$, donde $i = 1, \ldots, n$ es un índice que identifica a las observaciones (filas) en los datos.
Podríamos tener $n$ personas, familias o empresas en nuestra muestra.

La ecuación \eqref{eq:modelo_lineal_simple}, que sólo se diferencia de \eqref{eq:modelo_lineal_simple_poblacional} por ese pequeño subíndice $i$, es la versión \emph{muestral} del modelo lineal simple.
Como hemos supuesto que tenemos una muestra aleatoria, todavía podemos pensar que la variable dependiente $y_i$ es una función lineal de la variable independiente $x_i$, y que tenemos un término de error $\mu_i$ para capturar el efecto que otros factores tienen sobre $y_i$.
%Entonces intentaremos buscar ---o \emph{estimar}--- una combinación de parámetros $\beta_0$ y $\beta_1$ que se ajuste a los datos de nuestra muestra.

Por ejemplo, podríamos tener un vector $y_i$ con los salarios de $n=30$ personas, y modelarlo como una función lineal de los años de educación de esas personas, $x_i$. Resulta útil tener una base de datos concreta con estos vectores, por lo que usaremos una pequeña muestra de datos simulados.

<<message=FALSE>>=
# Leer datos CSV de internet
simdatos <- read_csv(
  "https://raw.githubusercontent.com/acarril/NER/master/datos/simdatos.csv")
# Imprimir las primeras observaciones de los datos
simdatos
@

\begin{Rbox}
\verb|read_csv()| permite leer bases de datos en formato CSV. Lo usamos para leer una base en línea y luego asignamos esos datos al objeto \verb|simdatos| (datos simulados).
Luego escribimos el nombre del objeto (\verb|simdatos|) para imprimirlo. Como es un \eng{tibble} (una base de datos), R imprime solamente las primeras 10 observaciones.
Esto es útil para hacernos una idea rápida de las variables de la base y los valores que éstas toman.
\end{Rbox}

En estos datos la variable \verb|y| contiene salarios por hora (en miles de pesos) y la variable \verb|x| contiene años de educación.
Cada fila representa una observación (en este caso, una persona) distinta.
Siempre es recomendable graficar los datos, ya que un gráfico entrega mucha información y revela patrones que son difíciles de ver en los datos. Creamos un gráfico simple con \paq{ggplot2}:

<<>>=
ggplot(simdatos, aes(x, y)) + geom_point()
@

\begin{Rbox}
\paq{ggplot2} es un paquete muy poderoso para crear gráficos de distintos tipos. Su principal función es \verb|ggplot()|, que permite definir los datos y las variables que usaremos (dentro de \verb|aes()|). Luego usamos \kw{funciones geom} para definir el tipo de gráfico; en este caso \verb|geom_point()| crea un gráfico de puntos.
\end{Rbox}

Vemos que existe una evidente relación lineal entre $x_i$ e $y_i$: a mayor nivel de educación parece haber mayor nivel de ingreso.
Esto es bueno para nosotros, ya que queremos ajustar los datos al modelo lineal que escribimos en \eqref{eq:modelo_lineal_simple}.
Cabe preguntarse ahora cuál es la manera óptima de elegir los parámetros que determinan dicha relación lineal, es decir, ¿cómo podemos elegir $\beta_0$ (el intercepto) y $\beta_1$ (la pendiente) para que nuestra predicción de $y_i$ sea lo mejor posible?

Es claro que podríamos elegir entre una infinita variedad de combinaciones de $\beta_0$ y $\beta_1$ para modelar los datos. Por ejemplo, el código de abajo simula 150 líneas con interceptos y pendientes ``razonables'', los que se grafican a continuación.

<<>>=
set.seed(314)
modelos <- tibble(
  beta1 = runif(150, -3, 1),
  beta2 = runif(150, -1, 1)
  )
ggplot(simdatos, aes(x, y)) +
  geom_point() +
  geom_abline(
    aes(intercept = beta1, slope = beta2),
    data = modelos, alpha = 1/4
  )
@

Si bien hay muchas líneas que claramente no se ajustan bien a los datos, varias otras sí lo hacen, y resulta difícil determinar a simple vista cuál es ``la mejor'' (asumiendo que tal línea existe).
Es evidente que necesitamos un criterio riguroso para elegir los parámetros $\beta_0$ y $\beta_1$ de manera óptima.
Esto es lo que veremos a continuación.


\section{Mínimos Cuadrados Ordinarios}
\label{sec:MCO}

El método de Mínimos Cuadrados Ordinarios (MCO) es una respuesta directa a la pregunta que planteamos recién: ¿qué criterio conviene usar para elegir el modelo lineal que se ajuste ``mejor'' a los datos?
Intuitivamente, la idea es minimizar el error de ajuste entre los datos reales contenidos en $y_i$ y los datos predichos por nuestra elección de $\beta_0$ y $\beta_1$, que denotaremos con $\hat y_i$.
%Intuitivamente, la idea es minimizar el error de ajuste entre la predicción de $y_i$ definida por nuestra elección de $\beta_0$ y $\beta_1$, y los valores reales de $y_i$.

%Geométricamente, el método de MCO equivale a minimizar la distancia vertical entre los puntos y la recta definida por el modelo.

Ahora formalizaremos esta intuición, y para esto necesitamos introducir un poco de notación nueva.
Recordemos que estamos trabajando con \eqref{eq:modelo_lineal_simple}, que es la versión muestral del modelo lineal simple (reimpresa a continuación para comodidad del lector):
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i + \mu_i.
\end{equation*}
Dada una muestra de datos, llamaremos $\hat\beta_0$ y $\hat\beta_1$ a nuestra estimación de los coeficientes.
Entonces la \kw{línea de regresión}, denotada por $\hat y_i$, está definida como
\begin{equation}
\hat y_i = \hat{\beta}_0 + \hat\beta_1 x_i.
\label{eq:linea_regresion}
\end{equation}
Algebraicamente, $\hat y_i$ es el vector que resulta de multiplicar el vector $x_i$ por el escalar $\hat\beta_1$ y luego sumarle $\hat\beta_0$.
Geométricamente, $\hat y_i$ es simplemente una recta que resulta de aplicar una transformación lineal a $x_i$, donde el intercepto y la pendiente dependen de los valores que hayamos elegido para $\hat\beta_0$ y $\hat\beta_1$ en \eqref{eq:linea_regresion}. La \autoref{fig:MCO} muestra una línea de regresión en azul.

<<'MCO', fig.cap="Línea de regresión $\\hat y_i$ en azul y residuo $\\hat \\mu_i$ en rojo", echo=FALSE, message=FALSE>>=
dist1 <- simdatos %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x = x + dodge,
    pred = -1 + x * 0.4
  )

ggplot(dist1, aes(x, y)) + 
  geom_abline(intercept = -1, slope = 0.4, colour = "blue") +
  geom_linerange(aes(ymin = y, ymax = pred), colour = "red") +
  geom_point(colour = "grey40")
@

Ahora, si tomamos la diferencia entre $y_i$ (los datos) y $\hat y_i$ (nuestra predicción) obtenemos $\hat\mu_i$, el \kw{residuo} de nuestra estimación:
\begin{align}
\hat \mu_i &= y_i - \hat y_i \notag \\
\hat \mu_i &= y_i - \hat\beta_0 + \hat\beta_1 x_i.
\label{eq:residuos}
\end{align}
Gráficamente el residuo corresponde a la distancia vertical entre cada punto (los datos $y_i$) y la recta (la predicción $\hat y_i$), dibujado con líneas rojas en la \autoref{fig:MCO}.
Es decir que para cada observación $i$ tenemos una medida de la magnitud del error que una elección particular de $\hat\beta_0$ y $\hat\beta_1$ producen.

Armados con esta notación podemos volver a la pregunta central: ¿Cómo elegimos de manera óptima los parámetros?
El método de MCO consiste en elegir $\hat\beta_0$ y $\hat\beta_1$ en \eqref{eq:residuos} tal que se minimice la suma de los residuos al cuadrado.\footnote{¿Por qué al cuadrado? Porque se asume que es irrelevante si el error es una subestimación o una sobreestimación, de forma que lo único que importa es la magnitud. ¿Por qué no se usa el valor absoluto entonces? Buena pregunta: \url{https://stats.stackexchange.com/q/46019/91358}. Básicamente, porque es algebraicamente más simple manejar valores cuadráticos, y no hace ninguna diferencia en el resultado final.}
Geométricamente esto es equivalente a minimizar la suma de las distancias verticales entre todos los puntos y la línea de regresión (las líneas rojas de la \autoref{fig:MCO}).
Matemáticamente el problema es
\begin{align}
\min_{\hat\beta_0,\hat\beta_1} & \sum_{i=1}^n \hat\mu_i^2 \notag \\
\Leftrightarrow \min_{\hat\beta_0,\hat\beta_1} & \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1 x_i)^2.
\label{eq:prob_min_MCO}
\end{align}

Al resolver esta minimización obtendremos los estimadores MCO.\footnote{La derivación paso a paso puede encontrarse, por ejemplo, en \textcite[cap. 2]{wooldridge_introductory_2016}.}
Estos corresponden a la elección óptima de $\hat\beta_0$ y $\hat\beta_1$ que hemos estado buscando:
\begin{align}
\hat \beta_1 &= \sum_{i=1}^n \frac{\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n (x_i - \bar x)^2} \notag \\
 &= \frac{\Cov (x,y)}{\Var(x)} \label{eq:MCO_beta1} \\
\hat \beta_0 &= \bar y - \hat\beta_1 \bar x, \label{eq:MCO_beta0}
\end{align}
donde $\bar x$ e $\bar y$ son los promedios de $x_i$ e $y_i$, $\Var(x)$ es la varianza muestral de $x$ y $\Cov(x,y)$ es la covarianza muestral de $x$ e $y$.

¡Hemos logrado nuestro objetivo!
Lo anterior significa que para encontrar los estimadores MCO y obtener la línea de regresión que mejor se ajuste a los datos solo tenemos que calcular cuatro cosas: $\bar y$, $\bar x$, $\Cov (x,y)$ y $\Var (x,y)$. ¡Hagámoslo ahora!

<<>>=
# Indicar base de datos a usar
attach(simdatos)
# Promedios de 'x' e 'y'
x.barra <- mean(x)
y.barra <- mean(y)
# Varianza de 'x' y covarianza entre 'x' e 'y'
var.x <- var(x)
cov.xy <- cov(x,y)
# Calcular parámetros MCO
beta1 <- cov.xy/var.x
beta0 <- y.barra - beta1*x.barra
# 'Desactivar' base de datos
detach(simdatos)
# Imprimir parámetros calculados
beta0
beta1
@

\begin{Rbox}
La función \verb|attach()| es útil para indicar que vamos a usar una base de datos en particular para todas las funciones que siguen. Entonces cuando calculamos el promedio de \verb|x| con \verb|mean(x)|, R sabe que se trata de la variable \verb|x| en el marco \verb|simdatos| (y no de \verb|votos|, por ejemplo). Luego usamos \verb|detach()| para ``desactivar'' el uso de la base que indicamos previamente.
\end{Rbox}

Vemos que es fácil calcular los estimadores MCO, y para este ejemplo particular obtuvimos que $\hat \beta_0 = \Sexpr{beta0}$ y $\hat \beta_1 = \Sexpr{beta1}$ (redondeando).
Con esto podemos completar la línea de regresión definida en \eqref{eq:linea_regresion}, usando nuestras estimaciones:
\begin{equation*}
\hat y_i = \Sexpr{beta0} + \Sexpr{beta1}x_i.
\end{equation*}
Esto está bien, pero no es demasiado práctico para visualizar lo que hemos logrado.
Una alternativa mejor es usar estos parámetros para graficar $\hat y_i$ sobre los datos:

<<>>=
# Graficar línea de regresión MCO
ggplot(simdatos, aes(x,y)) + 
  expand_limits(x = 0, y = -2.5) +
  geom_point() +
  geom_abline(
    intercept = beta0, slope = beta1, 
    color = "blue", show.legend = FALSE )
@

Finalmente, si bien resulta útil calcular explícitamente los estimadores para entender mejor qué es lo que estamos haciendo, esto no es demasiado práctico: estamos calculando y guardando promedios, varianzas y covarianzas que no nos interesan.
En realidad cualquier programa estadístico (decente) tiene algún método para estimar los parámetros de un modelo lineal de manera más compacta, y R no es la excepción:

<<>>=
# Estimar directamente los parámetros de un modelo lineal
lm(y ~ x, data = simdatos)
@

\begin{Rbox}
La función \verb|lm()| viene de \eng{linear model}, y permite calcular rápidamente los parámetros de un modelo lineal. El primer argumento de la función es una fórmula de R, que no es exactamente lo mismo que una fórmula en el sentido usual de la palabra. Por el momento basta entender que a la izquierda de \verb|~| indicamos la variable dependiente, mientras que a la derecha indicamos las variables independientes. Entonces \verb|lm()| toma una fórmula como \verb|y~x| y la traduce automáticamente a algo como \verb|y = beta0 + beta1 * x|.
\end{Rbox}

%Una de las ventajas de usar \verb|lm()| es que podemos guardar sus resultados en un objeto, el que luego podemos manipular para extraer otro tipo de información útil del modelo. A continuación veremos cómo extraer, manipular e interpretar esta información, lo que nos dará una comprensión mucho más profunda de lo que estamos haciendo.

Hemos calculado los parámetros del modelo, y sabemos lo que éstos quieren decir en términos matemáticos (minimizan el la suma de los errores cuadráticos) y geométricos (son el intercepto y la pendiente de la línea de ``mejor'' ajuste).
¿Pero pueden decirnos algo de los datos que usamos para calcularlos?
¿Qué significa, en términos de la relación entre educación e ingreso, que $\hat \beta_0 = \Sexpr{beta0}$ y $\hat \beta_1 = \Sexpr{beta1}$?
Saber interpretar correctamente los parámetros de una regresión es un arte, y comenzamos a explorarlo a continuación.


\section{Interpretando los parámetros estimados}

Una vez obtenidos los parámetros del modelo, nos interesa poder interpretarlos de manera útil. ¿Qué es lo que quiere decir que hayamos obtenido ciertos valores de $\hat\beta_0$ y $\hat\beta_1$, más allá de ser el intercepto y la pendiente de una ecuación?
Para explorar este tema dejaremos de usar datos simulados y pasaremos a datos reales.
En particular usaremos \verb|wage2|, que es una de las bases usadas en \textcite{wooldridge_introductory_2016} y contiene información de años de educación e ingresos para 935 personas.\footnote{Esta base es un subconjunto de los datos del estudio de \textcite{blackburn_unobserved_1992}.}

<<>>=
ingresos <- read_dta("http://fmwww.bc.edu/ec-p/data/wooldridge/wage2.dta")
ingresos <- select(ingresos, wage, educ)
@

\begin{Rbox}
Usamos \verb|haven::read_dta()| para leer bases de datos de Stata, como ya hicimos previamente. En la segunda línea, \verb|select()| nos permite seleccionar las variables \verb|wage| y \verb|educ| de la base \verb|ingresos| que acabamos de leer. Asignamos el resultado de \verb|select()| al mismo objeto para sobreescribirlo con solamente las variables que nos interesan.
\end{Rbox}

Sólo por conveniencia, podemos escribir la línea de regresión definida en \eqref{eq:linea_regresion} usando los nombres de las variables que nos interesan: salario mensual en dólares (\verb|wage|) y años de educación (\verb|educ|). Luego estimamos los coeficientes del modelo con \verb|lm()|, esta vez definiendo una fórmula donde \verb|wage| es la variable dependiente y \verb|educ| es la variable independiente:
\begin{align*}
\hat y &= \hat\beta_0 + \hat\beta_1 x \\
\Leftrightarrow \widehat{wage} &= \hat\beta_0 + \hat\beta_1 {educ}.
\end{align*}

<<'ingresos_lm', fig.cap="Modelo de regresión lineal relacionando años de educación (\\texttt{educ}) y salario (\\texttt{wage})", echo=1:2>>=
lm(wage ~ educ, data = ingresos)
ggplot(ingresos, aes(educ, wage)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
modelo <- lm(wage ~ educ, data = ingresos)
@
\begin{Rbox}
\verb|geom_smooth| es una función geom que podemos usar en conjunto con \verb|ggplot()| para graficar medias condicionales. Al usar la opción \verb|method = "lm"| indicamos que queremos graficar un modelo lineal entre las variables de los ejes. El uso de \verb|se=FALSE| es para desactivar los intervalos de confianza, tema del que hablaremos más adelante.
\end{Rbox}

El intercepto del modelo lineal ($\beta_0$) es el valor predicho de $y$ cuando $x=0$. En términos de nuestro ejemplo, si una persona tiene 0 años de educación el modelo predice un ingreso mensual promedio de \Sexpr{coefficients(modelo)[1]} dólares.
Es importante notar que en algunos contextos no tiene demasiado sentido pensar en $x=0$ (¿cuántas personas con 0 años de educación existirán? ¿cuántas hay en los datos?). En esas situaciones $\hat\beta_0$ no tiene mucho valor en sí mismo, y es labor del investigador discriminar los casos dónde esto ocurra.
Exploraremos este punto con mayor profundidad en otro ejemplo más adelante.

El coeficiente estimado para la pendiente ($\hat\beta_1$) nos dice la cantidad en la que cambia $\hat y$ cuando $x$ aumenta en una unidad. En otras palabras,
\begin{equation}
\Delta \hat y = \hat \beta_1 \cdot \Delta x,
\end{equation}
donde el operador $\Delta$ (``delta'') significa simplemente ``el cambio en ...''.

El aumento de $x$ puede ir asociado tanto a un aumento como a una disminución de $\hat y$.
Si $\hat\beta_1 > 0$ entonces un aumento de una unidad de $x$ está asociado a un aumento de $\hat\beta_1$ en $\hat y$; si $\hat\beta_1 < 0$ entonces un aumento de una unidad de $x$ está asociado a una disminución de $-\hat\beta_1$ en $\hat y$.
Por ejemplo, en la estimación graficada en la \autoref{fig:ingresos_lm} hemos obtenido que $\hat\beta_1 = \Sexpr{coefficients(modelo)[2]}$.
Esto puede interpretarse como que un aumento de un año de educación ($x$ aumenta en una unidad) está asociado a un aumento promedio de (casi) \Sexpr{round(coefficients(modelo)[2])} dólares en el salario.
Podemos asegurar que un aumento de $x$ se relaciona con un aumento en $\hat y$ porque el signo del coeficiente estimado es positivo; si el coeficiente estimado hubiese sido negativo, entonces diríamos que un año adicional de educación \emph{reduce} el salario promedio (¿tendría sentido esto?).

Hemos sido cuidadosos en no atribuirle una interpretación causal a la variable independiente. En otras palabras, evitamos decir que un año adicional de educación \emph{provoca} un aumento promedio de \Sexpr{round(coefficients(modelo)[2])} dólares en el ingreso. La pregunta de causalidad es importante, y un tema muy estudiado en econometría. Sin embargo, sólo podremos hablar de causalidad más adelante, cuando hayamos estudiado las propiedades estadísticas de los estimadores de MCO e impongamos supuestos más fuertes sobre la población.

\subsection{La unidad de medida}

Habrás notado que la unidad de medida de las variables es clave a la hora de interpretar los coeficientes estimados. En el ejemplo anterior $x$ son años de educación, y por lo tanto $\hat\beta_1 = \Sexpr{coefficients(modelo)[2]}$ implica que un año adicional de educación está asociado a un aumento de \Sexpr{coefficients(modelo)[2]} dólares en el salario mensual promedio, porque la unidad de $y$ son dólares mensuales.

Veamos ahora cómo cambian los parámetros estimados cuando cambia la unidad de medida de las variables.
En concreto, usaremos los datos \verb|mtcars|, que contiene 32 observaciones de modelos de auto.
Nos concentramos en una variable que indica su rendimiento en millas por galón (\verb|mpg|) y otra con su peso en miles de libras (\verb|wt|).

<<>>=
# Calcular estadística descriptiva para las variables 'mpg' y 'wt'
summary(select(mtcars, mpg, wt))
# Estimar el modelo lineal 
lm(mpg ~ wt, data = mtcars)
@

<<echo=FALSE>>=
modelo <- lm(mpg ~ wt, data = mtcars)
@


\begin{Rbox}
\verb|summary()| es una de las funciones más útiles de R base, ya que permite producir un resumen con información útil de distintos objetos: bases de datos, modelos, etc. En este caso la usamos para producir estadística descriptiva de las variables elegidas con \verb|select()|, para evitar usar la función sobre todas las variables de \verb|mtcars|.
\end{Rbox}

Al estimar este modelo obtenemos que $\hat\beta_1 = \Sexpr{coefficients(modelo)[2]}$. Esto significa que un aumento de una unidad en \verb|wt| está asociado a una disminución promedio de $\Sexpr{abs(coefficients(modelo)[2])}$ en \verb|mpg|.

OK, ¿y?

Como no sé mucho de autos ni del sistema métrico imperial, no tengo idea si esto es mucho o poco.
A pesar que entiendo cómo interpretar el coeficiente en términos algebraicos, la unidad de medida de las variables hace que esta información no sea tan útil (para mi).

Afortunadamente es muy fácil transformar las variables y luego volver a estimar el modelo con ellas. Crearemos las variables \verb|rendimiento_kml| con el rendimiento en kilómetros por litro y \verb|peso_kg| con el peso en kilógramos de cada auto:

<<>>=
# Crear marco 'autos' con las variables que nos interesan
autos <- as_tibble(select(rownames_to_column(mtcars, var = "modelo"), 
                          modelo, mpg, wt))
# Crear dos variables nuevas de rendimiento y peso
autos <- mutate(autos,
                rendimiento_kml =  mpg * 0.425144,
                peso_kg = wt * 453.592)
# Estimar el modelo con estas nuevas variables
lm(rendimiento_kml ~ peso_kg, data = autos)
@
<<echo=FALSE>>=
modelo <- lm(rendimiento_kml ~ peso_kg, data = autos)
@

Esta transformación todavía presenta un pequeño problema. Como la unidad de medida de \verb|peso_kg| son kilos, $\hat\beta_1 = \Sexpr{round(coefficients(modelo)[2],3)}$ nos dice que por cada kilo adicional el modelo predice una reducción de \Sexpr{abs(round(coefficients(modelo)[2],3))} en el rendimiento promedio.
Claramente pensar en un kilo adicional en algo tan pesado como un auto no tiene mucho sentido, por lo que estimaremos el modelo con una variable que indique el peso en unidades de 100 kilos:
<<>>=
# Agregar variable con peso en unidades de 100kg
autos <- autos %>% mutate(peso_100kg = peso_kg / 100)
# Estimar modelo con esta nueva variable independiente
lm(rendimiento_kml ~ peso_100kg, data = autos)
@
<<echo=FALSE>>=
modelo <- lm(rendimiento_kml ~ peso_100kg, data = autos)
@

\begin{Rbox}
Este bloque introduce el uso del \kw{operador de pipa}, \verb|%>%|.
Creado por Milton Bache en su paquete \paq{magrittr}, el operador de pipa sirve para pasar un objeto como primer argumento de una función.
Entonces en lugar de escribir \verb|f(x,y)| podemos escribir de manera equivalente \verb|x %>% f(y)|.
Esto permite ``desanidar'' funciones, ya que el operador es recursivo: operaciones como $f(g(x))$ pueden expresarse como \verb|x %>% g %>% f|, lo que las hace más legibles.
\end{Rbox}

Ahora podemos interpretar el modelo más fácilmente, teniendo unidades adecuadas.
El coeficiente asociado a la variable independiente se puede interpretar como que el modelo predice una reducción promedio de $\Sexpr{abs(coefficients(modelo)[2])}$ km/l por cada 100 kilos adicionales que pese un auto.
Esto definitivamente parece razonable, y los coeficientes entregan información útil directamente.
Es importante notar que este valor de $\hat\beta_1$ usando \verb|peso_100kg| es igual al calculado usando \verb|peso_kg|, pero multiplicado por 100. Al realizar una transformación lineal sobre la variable, transformamos de manera inversa el coeficiente asociado.

Por otro lado, el parámetro del intercepto indica que el modelo predice un rendimiento promedio de $\Sexpr{coefficients(modelo)[1]}$ para un auto de 0 kilos.
Este corresponde a un claro caso donde el intercepto no tiene sentido en sí mismo, ya que es imposible un auto de 0 kilos.

Recordemos que la línea de regresión de este modelo es
\begin{equation*}
\hat y_i = \Sexpr{coefficients(modelo)[1]} \Sexpr{coefficients(modelo)[2]} \cdot x_i.
\end{equation*}
Podemos usar la ecuación de la línea de regresión para calcular el valor predicho $\hat y_i$ dado un nivel de $x_i$. Por ejemplo, nuestra estimación predice que un auto de 1600 kilos tendrá un rendimiento promedio de $\Sexpr{coefficients(modelo)[1]} \Sexpr{coefficients(modelo)[2]} \cdot 16 = \Sexpr{coefficients(modelo)[1] + coefficients(modelo)[2] * 16}$ km/l.
Esto se ajusta bastante a los datos reales, como puede apreciarse en la \autoref{fig:autos_lm}.
%Como siempre, es conveniente dibujar esta línea $\hat y_i$ sobre los datos. El gráfico se muestra en la \autoref{fig:autos_lm}.

<<'autos_lm', fig.cap="Modelo de regresión lineal relacionando el peso de un auto (en unidades de 100 kilos) y su rendimiento (en kilómetros por litro)">>=
# Graficar la línea de regresión sobre los datos
autos %>% ggplot(aes(peso_100kg, rendimiento_kml)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
@



%\subsubsection{Errores estándar de los coeficientes}

%Los parámetros estimados ---los coeficientes de MCO, en este caso--- son estadísticos muestrales que usamos para hacer inferencias de los parámetros poblacionales. Es importante tener en cuenta que son estos parámetros poblacionales los que nos interesan en realidad, pero como no podemos observarlos directamente, debemos inferirlos por medio de una muestra finita.

%Es evidente que si tomáramos una muestra distinta a la actual, lo más probable es que estimaríamos parámetros diferentes. Si tomaras datos de otras 935 personas (o cualquier número, en realidad) es muy posible que al aplicar el mismo modelo obtuviéramos valores de $\beta_0$ y $\beta_1$ distintos de \Sexpr{coefficients(modelo)[1]} y \Sexpr{coefficients(modelo)[2]}. Además es muy probable que nuevamente ninguno de los dos sea igual al verdadero parámetro poblacional, que es el que nos interesa. Si continuáramos este proceso de tomar diferentes muestras y estimar los mismos parámetros una y otra vez, veríamos que la frecuencia relativa de las estimaciones obtenidas sigue una distribución de probabilidad, y por el Teorema Central del Límite sabemos que es probable que esta distribución sea normal. Entonces es importante para nuestra estimación que cuantifiquemos la cantidad de incertidumbre en esta distribución poblacional desconocida. Ahí es donde entra el concepto de error estándar, que corresponde a nuestra estimación de la desviación estándar de la distribución de estos muestreos. En términos intuitivos, es una medida de la incertidumbre de $\hat\beta$.

%Encerramos \verb|lm()| dentro de \verb|summary()| para obtener información más detallada de las estimaciones del modelo, incluyendo estimaciones para los errores estándar de los coeficientes:

%<<>>=
%summary(lm(wage ~ educ, data = ingresos))
%@

%todo: explicar mejor todos los summary de ols

%<<>>=
%ggplot(ingresos, aes(educ, wage)) + 
%  geom_point() + 
%  geom_smooth(method = "lm")
%@

\section{Distribución muestral de los estimadores}
\label{sec:distribucion_estimadores}

Es importante tener en cuenta que los estimadores MCO son eso: estimadores de un parámetro poblacional real.
Podemos pensar en MCO como una máquina en donde entra una muestra y salen estimadores, tal como se representa en la \autoref{fig:MCOcaja}.
Las estimaciones que realicemos dependen de la muestra que usemos, de forma que distintas muestran producirán distintas estimaciones de los parámetros.

%<<>>=
%library(hdm)
%AJR <- as_tibble(AJR)
%lm(GDP ~ logMort, data = AJR)
%ggplot(AJR, aes(logMort, GDP)) + geom_point() + geom_smooth(method = "lm")
%@

% Sacado de http://www.mattblackwell.org/
%\includegraphics[width=\textwidth]{figs/ols_blackbox}

\begin{figure}[htb]
\centering
\begin{tikzpicture}[thick]

\path [every node/.style={inner sep=0.2cm, outer sep=0.2cm}]
  node[anchor=east] (muestra1) at (0, 1.6cm) {Muestra 1: $\{ (y_1,x_1),\ldots,(y_n,x_n) \}$}
  node[anchor=east] (muestra2) at (0, 0.8cm) {Muestra 2: $\{ (y_1,x_1),\ldots,(y_n,x_n) \}$}
  node (muestradots) at (-4,0) {$\vdots$}
  node[anchor=east] (muestrak-1) at (0, -0.8cm) {Muestra $k-1$: $\{ (y_1,x_1),\ldots,(y_n,x_n) \}$}
  node[anchor=east] (muestrak) at (0, -1.6cm) {Muestra $k$: $\{ (y_1,x_1),\ldots,(y_n,x_n) \}$}
  [xshift=3cm]
  node[draw, minimum width=3cm, minimum height=3cm, anchor=center] (MCO) at (0,0) {MCO}
  [xshift=3cm]
  node[anchor=west] (est1) at (0, 1.6cm) {$(\hat\beta_0, \hat\beta_1)_{1}$}
  node[anchor=west] (est2) at (0, 0.8cm) {$(\hat\beta_0, \hat\beta_1)_{2}$}
  node[anchor=west] (estdots) at (1.15,0) {$\vdots$}
  node[anchor=west] (estk-1) at (0, -0.8cm) {$(\hat\beta_0, \hat\beta_1)_{k-1}$}
  node[anchor=west] (estk) at (0, -1.6cm) {$(\hat\beta_0, \hat\beta_1)_{k}$};

\begin{scope}[->,>=latex]
	% Entrada arriba
	\draw[->] (muestra1.east) to [out=0,in=140] ([yshift=1cm]MCO.west) ;
	\draw[->] (muestra2.east) to [out=0,in=140] ([yshift=.4cm]MCO.west) ;
	% Entrada abajo
	\draw[->] (muestrak-1.east) to [out=0,in=-140] ([yshift=-.4cm]MCO.west) ;
	\draw[->] (muestrak.east) to [out=0,in=-140] ([yshift=-1cm]MCO.west) ;
	% Salida arriba
	\draw[->] ([yshift=1cm]MCO.east) to [out=40,in=180] (est1.west) ;
	\draw[->] ([yshift=.4cm]MCO.east) to [out=40,in=180] (est2.west) ;
	% Salida abajo
	\draw[->] ([yshift=-.4cm]MCO.east) to [out=-40,in=-180] (estk-1.west) ;
	\draw[->] ([yshift=-1cm]MCO.east) to [out=-40,in=-180] (estk.west) ;
\end{scope}

\end{tikzpicture}
\caption{MCO es como una caja donde entran muestras y salen parámetros estimados}
\label{fig:MCOcaja}
\end{figure}

No es necesario creer en mi palabra. A continuación simulamos 500 muestreos aleatorios de 1000 observaciones cada uno. Nos concentraremos únicamente en el parámetro de la pendiente, $\beta_1$. Fijamos el verdadero valor de dicho parámetro en $\beta_1=0.7$ (en el código \verb|b <- 0.7|). Al simular los datos tomados de una población normal, usando los parámetros que hemos definido, estimamos el parámetro $k$ veces y obtenemos $k$ valores para $\hat\beta_1$, los que vamos guardando en el vector \verb|beta1|. Finalmente hacemos un gráfico de densidad con los valores de dicho vector.

% Simulacion de http://www.unc.edu/~carsey/teaching/ICPSR-2011/Sim%20Slides%20Handout.pdf
<<>>=
set.seed(1102) # fijar la semilla para resultados replicables

# Parámetros poblacionales ("verdaderos")
a <- 0.2 # verdadero valor del intercepto
b <- 0.7 # verdadero valor de la pendiente

# Parámetros de la simulación
k <- 500 # número de simulaciones 
n <- 1000 # tamaño muestral

# Comenzar simulación
beta1 <- numeric(length = k) # vector vacío para guardar pendientes estimadas
for(i in 1:k) { # loop de 500 simulaciones
  x <- rnorm(n, mean=2, sd=1) # variable independiente
  u <- rnorm(n, mean=0, sd=1) # término de error
  y <- a + b*x + u # variable dependiente
  estimacion <- lm(y ~ x) # estimar parámetros por MCO
  beta1[i] <- coef(estimacion)[2] # guardar pendiente (2do coef) estimada
}

# Graficar densidad de 500 pendientes estimadas
ggplot(tibble(beta1), aes(beta1)) +
  geom_vline(xintercept = b) +
  geom_density()
@

Lo que este gráfico nos muestra es que a pesar de que existe un único valor poblacional para uno de los parámetros estimados ($\beta_1$ en este caso), al estimar una regresión por MCO con distintas muestras obtendremos distintos valores estimados para dicho parámetro.
En un caso real no conoceremos el valor poblacional del parámetro, ni podremos tomar \Sexpr{k} muestras distintas... ¡tendremos suerte si obtenemos 1 buena muestra! Por eso, es esencial tener presente esta distribución de $\hat\beta$ al estimar un modelo.

Habrás notado que al tomar \Sexpr{k} muestras y estimar $\hat\beta_1$, los valores estimados se concentran en torno al verdadero valor de $\beta_1$, que es \Sexpr{b} en este ejemplo. ¡Esto es bueno... y no es casualidad! En realidad es una propiedad de los estimadores MCO.
A continuación formalizaremos esta y otras propiedades, con el fin de entender qué debe ocurrir para que se cumplan.

\section{Propiedades de los estimadores MCO}

Hasta ahora hemos aprendido a calcular e interpretar los parámetros de un modelo lineal simple.
%En la \autoref{sec:distribucion_estimadores} enfatizamos que nuestra estimación de $\hat\beta_0$ y $\hat\beta_1$ proviene de una distribución, ya que depende de la muestra con la que trabajemos.
%Entendemos entonces a $\hat\beta_0$ y $\hat\beta_1$ como estimadores de los verdaderos parámetros poblacionales, $\beta_0$ y $\beta_1$.
%Con esto en mente estudiaremos las propiedades de la distribución de $\hat\beta_0$ y $\hat\beta_1$ bajo distintas muestras aleatorias.
En la \autoref{sec:distribucion_estimadores} enfatizamos que los estimadores MCO ($\hat\beta_0$ y $\hat\beta_1$, o en general $\hat\beta$) tienen una distribución de probabilidad.
Intuitivamente, la razón por la que esto ocurre es porque nuestra estimación de $\hat\beta_0$ y $\hat\beta_1$ depende de la muestra aleatoria particular que tengamos de la población; si tomáramos distintas muestras, obtendríamos distintos valores para $\hat\beta$.
Entonces nuestra estimación de $\hat\beta$ es solo una realización de infinidad de posibilidades, como se muestra en la \autoref{fig:parametros_distribucion}.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
    no markers, domain=0:8, samples=100,
    axis lines*=left, xlabel=, ylabel=$f(\hat\beta)$,
    every axis y label/.style={at=(current axis.above origin),anchor=south},
    every axis x label/.style={at=(current axis.right of origin),anchor=west},
    height=2.4in, width=3.2in,
    xtick={3},
    xticklabels={$\hat\beta$},
    ytick=\empty,
    enlargelimits=false, clip=false, axis on top,
    grid = major
    ]
    \addplot [very thick, black] {gauss(4,1)};
    \end{axis}
  \end{tikzpicture}
  \caption{La realización de $\hat\beta$ que nosotros observamos proviene de una función de probabilidad}
  \label{fig:parametros_distribucion}
\end{figure}%

\subsection{Estimadores insesgados}

Teniendo en cuenta que los los estimadores tienen una función de densidad de probabilidad, parece natural querer que esta probabilidad al menos esté centrada en torno al verdadero parámetro $\beta$, es decir, que
\begin{equation}
\E(\hat \beta) = \beta.
\end{equation}
A un estimador que cumple con esta propiedad se le llama \kw{estimador insesgado}.
La \autoref{fig:estimadores_sesgo} muestra la distribución de un estimador insesgado en azul, mientras que la distribución de un estimador sesgado está graficada en rojo.
Notar que ambos tienen igual varianza, pero sólo el primero está centrado en torno al verdadero valor de $\beta$, y por lo tanto es insesgado.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
    no markers, domain=0:10, samples=100,
    axis lines*=left, xlabel=, ylabel=$f(\hat\beta)$,
    every axis y label/.style={at=(current axis.above origin),anchor=south},
    every axis x label/.style={at=(current axis.right of origin),anchor=west},
    height=2.4in, width=3.2in,
    xtick={4},
    xticklabels={$\beta$},
    ytick=\empty,
    enlargelimits=false, clip=false, axis on top,
    grid = major
    ]
    \addplot [very thick,cyan!50!black] {gauss(4,1)};
    \addplot [very thick,magenta!50!black] {gauss(6,1)};
    \end{axis}
  \end{tikzpicture}
  \caption{Estimador insesgado (azul) vs. estimador sesgado (rojo)}
  \label{fig:estimadores_sesgo}
\end{figure}%

Para asegurar que los estimadores MCO que hemos obtenido son insesgados deben cumplirse cuatro supuestos.
Hemos enunciado estos supuestos a medida que construimos los estimadores, pero a continuación los enunciaremos de forma más explícita.

\paragraph{RLS.1: Lineal en parámetros} Esto corresponde al supuesto fundamental del modelo, donde asumimos que las variables poblacionales se relacionan siguiendo la siguiente forma funcional:
\begin{equation}
y = \beta_0 + \beta_1 x + \mu. \tag{\ref{eq:modelo_lineal_simple_poblacional}, revisitada}
\end{equation}
Nuestro objetivo es estimar $\beta_0$ y $\beta_1$ suponiendo que los datos se relacionan de forma lineal en parámetros y que $x$, $y$ y $\mu$ son variables aleatorias.

\paragraph{RLS.2: Muestreo aleatorio} Suponemos que tenemos una muestra aleatoria de tamaño $n$ de la población total, de forma que en términos muestrales el modelo lineal es
\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \mu_i, \quad i = 1,\ldots,n. \tag{\ref{eq:modelo_lineal_simple}, revisitada}
\end{equation}
Sí, es lo mismo que arriba, pero con el subíndice. En términos de notación la diferencia es sutil; lo importante es entender lo que esta diferencia significa.

Por ejemplo, supongamos que queremos estimar el efecto del peso de un auto sobre su rendimiento en carretera. Es decir que el modelo a estimar es
\begin{equation*}
rendimiento_i = \beta_0 + \beta_1 peso_i + \mu_i,
\end{equation*}
donde nos interesa en particular la estimación de $\beta_1$.

Para efectos de este ejemplo supondremos que los datos de la población completa de automóviles del país se encuentra en la base \verb|mpg|. Usaremos la cilindrada del auto (en litros) como aproximación de su peso (\verb|displ|), y su rendimiento en carretera (en millas por galón) como variable independiente (\verb|hwy|). Estimar el coeficiente de interés es muy simple:

<<>>=
# Estimamos modelo "poblacional"
lm(hwy ~ displ, data = mpg)

# Guardamos el beta poblacional para usarlo en gráfico
beta_pob <- coefficients(lm(hwy ~ displ, data = mpg))[2]
@

Como supusimos que el total de observaciones en \verb|mpg| corresponde a la población completa, para efectos de este ejemplo el parámetro poblacional "verdadero" es $\beta_1 = \Sexpr{beta_pob}$.

Intentaremos ahora estimar dicho parámetro usando dos muestra tomadas de los datos poblacionales. Una muestra será completamente aleatoria, mientras que la otra sólo se extraerá del subconjunto de automóviles del año 1999. Ambas muestras serán del mismo tamaño (50 observaciones), y repetiremos este muestreo la misma cantidad de veces para ambos tipos de muestra (300 muestreos).

<<message=FALSE, warning=FALSE>>=
# Parámetros iniciales
set.seed(1102)
k <- 300

# Guardar matriz de coeficientes bajo distintas muestras
betas <- matrix(data=NA, nrow=k, ncol=2)
for(i in 1:k) {
  autos <- sample_n(mpg, size = 50)
  autos99 <- sample_n(filter(mpg, year==1999), size = 50)
  betas[i,1] <- coef(lm(hwy ~ displ, data = autos))[2]
  betas[i,2] <- coef(lm(hwy ~ displ, data = autos99))[2]
}

# Transformar matriz a base de datos usable para graficar
betas <- melt(betas)
betas <- mutate(betas, año = factor(Var2, labels = c("todos","1999")))

# Graficar densidad de coefs. estimados por tipo de muestra
ggplot(betas, aes(value)) +
  geom_vline(xintercept = beta_pob) +
  geom_density(aes(color = año)) +
  scale_x_continuous(name = expression(hat(beta)), limits = c(-5, -2.5))
@

Al tomar una muestra aleatoria de la población, la distribución de $\hat\beta_1$ se centra en torno al verdadero valor de $\beta_1$. Por otro lado, al tomar una muestra que no es aleatoria (en el ejemplo la muestra es condicional al año), la distribución de $\hat\beta_1$ no estará centrada en torno al verdadero valor del parámetro, es decir, es un estimador sesgado. En este caso particular hay un sesgo a la baja, ya que es de esperar que si sólo seleccionamos autos relativamente antiguos, estos serán en promedio menos eficientes.

<<message=FALSE>>=
# Cargar paquete para melt() y fijar semilla
set.seed(1102)

# Definir función para guardar pendientes simuladas
simbetas <- function(k, a=0.2, b=0.7, n=500) {
  beta1 <- numeric(length = k)
  for(i in 1:k) {
    x <- rnorm(n, mean=2, sd=1)
    u <- rnorm(n, mean=0, sd=1)
    y <- a + b*x + u
    estimacion <- lm(y ~ x)
    beta1[i] <- coef(estimacion)[2]
  }
  beta1
}

# Aplicar la función con distintos números de simulaciones
nsims <- c(20,60,400)
betas <- lapply(nsims, simbetas)

# Transformar datos para graficarlos
betas <- melt(betas)
betas <- mutate(betas, nsims = factor(L1, labels = nsims))

# Graficar densidad de coeficientes estimados por n. de sims 
ggplot(betas, aes(value)) +
  geom_vline(xintercept = .7) +
  geom_density(aes(color = nsims)) + 
  labs(x = expression(hat(beta)))
@


\paragraph{RLS.3: Variación en las variables independientes}
Necesitamos que la (o las) variables independientes del modelo tengan algo de variación.
Este supuesto es fácil, pero necesario matemáticamente.
Por ejemplo, supongamos que queremos estimar un modelo como
\begin{equation*}
nota_i = \beta_0 + \beta_1 estudio_i + \mu_i,
\end{equation*}
donde $estudio_i$ son los minutos estudiados por el alumno $i$ y $nota_i$ es su nota en una prueba.
Para estimar este modelo es necesario que exista variabilidad en la cantidad de horas estudiadas por los alumnos de la muestra. Si todos estudiaron exactamente la misma cantidad de minutos, entonces será imposible determinar la relación entre minutos estudiados y la nota obtenida. Este hecho también es obvio geométricamente:

<<>>=
data <- tibble(
  nota = rnorm(50, 4.6, 1), 
  estudio = rep(40, 50))
ggplot(data, aes(estudio, nota)) + 
  geom_point()
@

¿Cuál es la la línea que mejor se ajusta a los datos? \footnote{Y recuerda que una función $f: X\to Y$ corresponde a cualquier subconjunto $S$ del conjunto $X\times Y$ que satisface que si $(x,y)\in S$ y $(x,y')\in S$, entonces $y = y'$. O, en otras palabras, una función válida asigna a cada elemento de $X$ exactamente un elemento en $Y$. O, en otras palabras, una función no puede ser una línea vertical.}

\paragraph{RLS.4: Media condicional cero}
El error $u$ debe tener un valor esperado igual a cero, condicional a los valores de las variables independientes:
\begin{equation}
E(\mu | x) = 0.
\end{equation}
En una muestra aleatoria esto significa que
\begin{equation*}
E(\mu_i | x_i) = 0 \quad \forall\; i = 1,\ldots, n.
\end{equation*}

Hemos supuesto que la esperanza del error es cero, es decir, que $\E(\mu)=0$.
Mencionamos que no perdemos nada al hacer esto, ya que siempre podemos acomodar el intercepto de manera de asegurarnos que $\E(\mu)=0$. Entonces este supuesto es muy poco restrictivo.
El supuesto clave es asumir que $\mu$ y $x$ son independientes, de forma que $\E(\mu | x) = E(x) = 0$.
Sin embargo, si existe una correlación entre $\mu$ y $x$ esto implicará que $\E(\mu | x) \neq 0$.

Este es probablemente el supuesto más importante a la hora de determinar si una regresión lineal simple entregará parámetros insesgados. Esto ocurre porque es imposible determinar si el supuesto efectivamente se cumple o no, ya que depende de que hayamos especificado la forma funcional correcta y no omitido variables relevantes.

%todo: completar! Berry (1993) https://www.quora.com/In-linear-regression-why-we-use-the-assumption-of-zero-conditional-mean-rather-than-independency-between-x-and-error-term-along-with-E-u-0

\hrulefill

Usando los supuestos RLS.1 a RLS.4 se puede demostrar que los estimadores MCO son insesgados, es decir, que\footnote{La demostración completa corresponde al Teorema 2.1 de \textcite[cap. 2]{wooldridge_introductory_2016}.}
\begin{equation}
\E(\hat\beta_0) = \beta_0 \text{ y } \E(\hat\beta_1) = \beta_1.
\end{equation}

En general, si cualquiera de los supuestos no se cumple entonces perderemos la propiedad de insesgamiento. RLS.1 requiere que $y$ y $x$ estén relacionados linealmente, lo que ciertamente puede no cumplirse. Hay que recordar, sin embargo, que la relación es solamente lineal en \emph{parámetros}, no en variables, por lo que ciertamente podemos transformar $x$ e $y$ para capturar relaciones no lineales más interesantes.
RLS.2 asume que nuestros datos corresponden a una muestra aleatoria de la población, lo que claramente no siempre se cumple en datos de corte transversal. Por ejemplo, es posible que solamente tengamos datos de micro y pequeñas empresas, o que solamente observemos jefas de hogar de los quintiles más pobres.

RLS.3 es extremadamente básico y con mucha seguridad se cumplirá en cualquier tipo de datos. Sin embargo, RSL.4 ciertamente representa un problema potencial. Si no se cumple que la media condicional del error es 0, esto es equivalente a decir que existe alguna correlación entre $x$ y $\mu$.
Una razón por la que esto puede ocurrir es porque dejamos de incluir una variable relevante en nuestro modelo, de forma que esta afecta a $x$ por medio de $\mu$.

\begin{figure}[htb]
\centering
\begin{subfigure}{.55\textwidth}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
    no markers, domain=0:10, samples=100,
    axis lines*=left, xlabel=, ylabel=$f(\hat\beta)$,
    every axis y label/.style={at=(current axis.above origin),anchor=south},
    every axis x label/.style={at=(current axis.right of origin),anchor=west},
    height=2.4in, width=3.2in,
    xtick={4},
    xticklabels={$\beta$},
    ytick=\empty,
    enlargelimits=false, clip=false, axis on top,
    grid = major
    ]
    \addplot [very thick,cyan!50!black] {gauss(4,1)};
    \addplot [very thick,magenta!50!black] {gauss(6,1)};
    \end{axis}
  \end{tikzpicture}
  \caption{Estimador insesgado (azul) vs. estimador sesgado (rojo)}
%  \label{fig:estimadores_sesgo}
\end{subfigure}%
\begin{subfigure}{.55\textwidth}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
    no markers, domain=0:10, samples=100,
    axis lines*=left, xlabel=, ylabel=$f(\hat\beta)$,
    every axis y label/.style={at=(current axis.above origin),anchor=south},
    every axis x label/.style={at=(current axis.right of origin),anchor=west},
    height=2.4in, width=3.2in,
    xtick={5},
    xticklabels={$\beta$},
    ytick=\empty,
    enlargelimits=false, clip=false, axis on top,
    grid = major
    ]
    \addplot [very thick,cyan!50!black] {gauss(5,0.5)};
    \addplot [very thick,magenta!50!black] {gauss(5,1.2)};
    \end{axis}
  \end{tikzpicture}
  \caption{Estimadores insesgados con distinta varianza}
  \label{fig:estimadores_varianza}
\end{subfigure}
\caption{Valor esperado y varianza de los estimadores}
\label{fig:estimadores_OLS_propiedades}
\end{figure}

Además de requerir que el estimador se centre en torno al verdadero valor del parámetro, también queremos saber qué tan lejos podemos esperar que $\hat\beta$ se aleje de $\beta$ en promedio. Esto nos permitirá elegir el estimador más eficiente, es decir, de entre todos los estimadores insesgados elegir aquél que tenga la menor varianza.
La \autoref{fig:estimadores_varianza} muestra la distribución de dos estimadores insesgados. Sin embargo, el estimador representado por la curva roja tiene relativamente mayor varianza que el representado por la azul.

Nuestro objetivo es obtener ambas propiedades para nuestros estimadores MCO.
Sin embargo, estas propiedades sólo existirán en la medida que se cumplan algunos supuestos. En esta sección veremos cuáles son estos supuestos y qué propiedades entregan.

Es importante tener en cuenta que las propiedades estadísticas no tienen nada que ver con una muestra en particular, si no que es una propiedad que los estimadores cumplirán cuando se realice un muestreo aleatorio repetidamente.


\subsection{Valor esperado de los estimadores}

Hasta ahora hemos aprendido a calcular e interpretar los parámetros de un modelo lineal simple. Sin embargo, recordemos que nuestra estimación de $\hat\beta_0$ y $\hat\beta_1$ proviene de una muestra aleatoria de la población.
En esta sección analizaremos las propiedades estadísticas de $\hat\beta_0$ y $\hat\beta_1$, lo que nos dará herramientas para analizar qué tanto se acercan nuestras estimaciones a los verdaderos parámetros poblacionales.

Primero nos detendremos a entender el problema en sí: ¿en qué se diferencian los parámetros estimados ($\hat\beta_0$ y $\hat\beta_1$) de los verdaderos parámetros poblacionales ($\beta_0$ y $\beta_1$)?

Cuando planteamos el modelo de regresión lineal simple (ecuación \eqref{eq:modelo_lineal_simple_poblacional}), supusimos que las variables $x$ e $y$ se relacionaban por medio de una función lineal, y que 

Si bien en estricto rigor suponemos que el ``tamaño'' de la población es infinito, fijamos $N=1.000.000$ para efectos de nuestra población simulada.
Luego definimos un error \verb|u| con media 0 (es uno de nuestros supuestos) y desviación estándar \verb|sigma| $=5$. Finalmente generamos \verb|y| como una función lineal de \verb|x|, con intercepto 2 y pendiente 10, e incluyendo nuestro error \verb|u|$\sim \mathcal{N}(0, 5)$.\footnote{No es necesario asumir que $\mu \sim \mathcal{N}$, ya que bastaría con que $\E(\mu)=0$. Es un error común pensar que MCO requiere errores con distribución normal.}

<<>>=
# Parámetros iniciales
set.seed(03072017)
N <- 1000000
sigma <- 5

# Variables poblacionales
x <- runif(N)
u <- rnorm(N, 0, sigma)
y <- 2 + 10*x + u
poblacion <- data.frame(y,x)
@

Al haber creado nosotros mismos los datos poblacionales, estamos seguros de los verdaderos valores de los parámetros: $\beta_0=2$ y $\beta_1=10$. La única desviación que se dará en nuestra simulación es debido a que la población no es infinita (aunque se le acerca bastante).

<<>>=
# Estimar parámetros poblacionales
lm(y~x, data = poblacion)
@

Verificamos que en nuestra población simulada los parámetros son ...
A continuación extraemos una muestra aleatoria de 100 observaciones ($n=100$) de la población. Esto es lo que nosotros suponemos que hacemos cuando tenemos, por ejemplo, una base de datos con puntajes SIMCE para de 100 estudiantes.
Luego estimamos los coeficientes $\hat\beta_0$ y $\hat\beta_1$ para esta muestra en particular.

<<echo=-4>>=
# Extraer una muestra aleatoria de la población y estimar sus parámetros
muestra <- poblacion[sample(1:N, 100),]
lm(y~x, data = muestra)
modelo_muestral <- lm(y~x, data = muestra)
coefficients(modelo_muestral)
@

Observamos que los coeficientes estimados en esta muestra particular son bastante distintos de los reales: $\hat\beta_0 = \Sexpr{coefficients(modelo_muestral)[1]}$ y $\hat\beta_1 = \Sexpr{coefficients(modelo_muestral)[2]}$.
Este fenómeno no es exclusivo a los estimadores, y en realidad ocurre para cualquier tipo de estadística que es calculada para la muestra de una población.

Por ejemplo, supón una población (un poco ridícula) de 4 personas, y observamos sus pesos en kilogramos:
\begin{equation*}
P = \{74, 62, 65, 71\}.
\end{equation*}
Si tomamos muestras de 2 personas repetidas veces, obtendremos distintas estimaciones del promedio poblacional. Algunos promedios muestrales serán iguales al promedio poblacional, mientras que otros no. Lo importante es que al tomar varias muestras y guardar registro de los promedios muestrales,

ninguna muestra de menos de 2 observaciones nos entregará este promedio. Al extraer repetidas muestras obtendremos una distribución muestral: algunos promedios de nuestra muestra se repetirán más que otros.

\hrulefill



\section{Varianza de los estimadores}

Además de asegurarnos que la distribución muestral de $\hat\beta$ esté centrada alrededor de $\beta$ ---es decir, que $\hat\beta$ es insesgado---, es importante saber qué tan dispersa es esta distribución de $\hat\beta$.
La \autoref{fig:estimadores_varianza} muestra el caso de dos estimadores insesgados donde uno tiene una distribución con mayor varianza que el otro.
Parece lógico querer elegir, de entre todos los estimadores insesgados, aquel con menor varianza (ie. el azul).

Para poder calcular la varianza de los estimadores MCO es necesario agregar un último supuesto:

\paragraph{RLS.5: Homocedasticidad}\index{homocedasticidad} 
Asumimos que el error $\mu$ tiene la misma varianza condicional para todos los valores de la variable independiente. Es decir,
\begin{equation}
\Var(\mu | x) = \sigma^2.
\end{equation}

Usando los supuestos SLR.1 a SLR.5 podemos demostrar que

\begin{align}
\Var(\hat\beta_0) &= \frac{\sigma^2/n \sum_{i=1}^n x_i^2}{\sum_{i=1}^n (x_i - \overline x)^2} \label{eq:var_hat_beta0} \\
\Var(\hat\beta_1) &= \frac{\sigma^2}{\sum_{i=1}^n (x_i - \overline x)^2}.\label{eq:var_hat_beta1}
\end{align}

Usualmente nos interesará $\Var(\hat\beta_1)$.
Vemos que esta varianza depende positivamente de la varianza del error $\sigma^2$. Esto tiene sentido, ya que a mayor varianza de los factores no observables que afectan $y$, es más difícil estimar con precisión $\beta_1$.
Por otro lado, $\Var(\hat\beta_1)$ depende negativamente de la variación total de $x$, representada por $SST_x = \sum_{i=1}^n (x_i - \overline x)^2$. Esto es así porque entre más dispersos estén los valores de la variable independiente, más fácil será detectar la relación entre $\E(y|x)$ y $x$.
Además, a medida que el tamaño muestral aumente, también lo hará la variación total en $x_i$, de forma que una muestra más grande siempre resultará en una menor varianza de $\hat\beta_1$.

Cuando $\Var(\mu | x)$ depende de $x$ (cambia con $x$) decimos que el término de error presenta \kw{heterocedasticidad}, o varianza no constante.
La heterocedasticidad es un problema que es más difícil de pronunciar que de entender: ocurre cuando la dispersión de una variable dependiente no es constante para distintos valores de la variable independiente.

La \autoref{fig:heterocedasticidad} muestra datos heterocedásticos para el caso de una regresión bivariada, tanto para una variable dependiente discreta como continua.
En el eje $f(y|x)$ se grafica la densidad del término de error, la que en este ejemplo claramente claramente disminuye a medida que $x$ es mayor.

\begin{figure}[htb]
\pgfmathsetseed{112}
\centering
\begin{subfigure}{.5\textwidth}
\makeatletter
        \pgfdeclareplotmark{dot}
        {%
            \fill circle [x radius=0.08, y radius=0.32];
        }%
\makeatother
  \centering
  \begin{tikzpicture}[ % Define Normal Probability Function
declare function={
            normal(\x,\m,\s) = 1/(2*\s*sqrt(pi))*exp(-(\x-\m)^2/(2*\s^2));
        },
    declare function={invgauss(\a,\b) = sqrt(-2*ln(\a))*cos(deg(2*pi*\b));}
       ]
\begin{axis}[
    %no markers,
    domain=0:12,
    zmin=0, zmax=1,
    xmin=0, xmax=3,
    samples=200,
    samples y=0,
    view={40}{30},
    axis lines=middle,
    enlarge y limits=false,
    xtick={0.5,1.5,2.5},
    xmajorgrids,
    xticklabels={},
    ytick=\empty,
%   xticklabels={$x_1$, $x_2$, $x_3$},
    ztick=\empty,
    xlabel=$x$, xlabel style={at={(rel axis cs:1,0,0)}, anchor=west},
    ylabel=$y$, ylabel style={at={(rel axis cs:0,1,0)}, anchor=south west},
    zlabel=$f(y|x)$, zlabel style={at={(rel axis cs:0,0,1)}, anchor=south},
    set layers, mark=cube
  ]

\pgfplotsinvokeforeach{0.5,1.5,2.5}{
\addplot3 [draw=none, fill=black, opacity=0.25, only marks, mark=dot, mark layer=like plot, samples=30, domain=0.1:2.9, on layer=axis background] (#1, {1.5*(#1-0.5)+3+invgauss(rnd,rnd)*#1}, 0);
}
\addplot3 [samples=2, samples y=0, domain=0:3] (x, {1.5*(x-0.5)+3}, 0);
\addplot3 [blue!50, thick] (0.5, x, {normal(x, 3, 0.5)});
\addplot3 [blue!50, thick] (1.5, x, {normal(x, 4.5, 1)});
\addplot3 [blue!50, thick] (2.5, x, {normal(x, 6, 1.5)});

\pgfplotsextra{
\begin{pgfonlayer}{axis background}
\draw [gray, on layer=axis background] (0.5, 3, 0) -- (0.5, 3, {normal(0,0,0.5)}) (0.5,0,0) -- (0.5,12,0)
    (1.5, 4.5, 0) -- (1.5, 4.5, {normal(0,0,1)}) (1.5,0,0) -- (1.5,12,0)
    (2.5, 6, 0) -- (2.5, 6, {normal(0,0,1.5)}) (2.5,0,0) -- (2.5,12,0);

\end{pgfonlayer}
}
\end{axis}

\end{tikzpicture}
  \caption{Variable dependiente discreta}
  \label{fig:heterocedasticidad_x_discreta}
\end{subfigure}%
\makeatletter
        \pgfdeclareplotmark{dot}
        {%
            \fill circle [x radius=0.02, y radius=0.08];
        }%
\makeatother
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tikzpicture}[ % Define Normal Probability Function
declare function={
            normal(\x,\m,\s) = 1/(2*\s*sqrt(pi))*exp(-(\x-\m)^2/(2*\s^2));
        },
    declare function={invgauss(\a,\b) = sqrt(-2*ln(\a))*cos(deg(2*pi*\b));}
       ]
\begin{axis}[
    %no markers,
    domain=0:12,
    zmin=0, zmax=1,
    xmin=0, xmax=3,
    samples=200,
   samples y=0,
    view={40}{30},
    axis lines=middle,
    enlarge y limits=false,
    xtick={0.5,1.5,2.5},
    xmajorgrids,
    xticklabels={},
    ytick=\empty,
%   xticklabels={$x_1$, $x_2$, $x_3$},
    ztick=\empty,
    xlabel=$x$, xlabel style={at={(rel axis cs:1,0,0)}, anchor=west},
    ylabel=$y$, ylabel style={at={(rel axis cs:0,1,0)}, anchor=south west},
    zlabel=$f(y|x)$, zlabel style={at={(rel axis cs:0,0,1)}, anchor=south},
    set layers, mark=cube
  ]

\addplot3 [gray!50, only marks, mark=dot, mark layer=like plot, samples=200, domain=0.1:2.9, on layer=axis background] (x, {1.5*(x-0.5)+3+invgauss(rnd,rnd)*x}, 0);
\addplot3 [samples=2, samples y=0, domain=0:3] (x, {1.5*(x-0.5)+3}, 0);
\addplot3 [blue!50, thick] (0.5, x, {normal(x, 3, 0.5)});
\addplot3 [blue!50, thick] (1.5, x, {normal(x, 4.5, 1)});
\addplot3 [blue!50, thick] (2.5, x, {normal(x, 6, 1.5)});

\pgfplotsextra{
\begin{pgfonlayer}{axis background}
\draw [gray, on layer=axis background] (0.5, 3, 0) -- (0.5, 3, {normal(0,0,0.5)}) (0.5,0,0) -- (0.5,12,0)
    (1.5, 4.5, 0) -- (1.5, 4.5, {normal(0,0,1)}) (1.5,0,0) -- (1.5,12,0)
    (2.5, 6, 0) -- (2.5, 6, {normal(0,0,1.5)}) (2.5,0,0) -- (2.5,12,0);

\end{pgfonlayer}
}
\end{axis}


\end{tikzpicture}
  \caption{Variable dependiente continua}
  \label{fig:heterocedasticidad_x_continua}
\end{subfigure}
\caption{Heterocedasticidad}
\label{fig:heterocedasticidad}
\end{figure}

Podemos generar fácilmente un conjunto de datos que presenten heterocedasticidad:

<<fig.width=4.5, fig.height=4.5>>=
set.seed(314)
n <- 256                           # Número de observaciones
x <- (1:n)/n                       # Valores de `x`
e <- rnorm(n, sd=1)                # Valores aleatorios de una normal
i <- order(runif(n, max=dnorm(e))) # Ubicamos los más grandes al final
y <- 1 + 5 * x + e[rev(i)]         # Generamos `y` con el error `e`.
modelo <- lm(y ~ x)                # Guardamos el modelo lineal
plot(x, y)
abline(coef(modelo), col = "Red")
@

En presencia de heterocedasticidad una estimación por MCO seguirá entregando coeficientes consistentes e insesgados. Sin embargo, no podremos estimar correctamente la matriz de varianzas-covarianzas, lo que producirá que los errores estándar de los coeficientes estén sesgados. Esto conduce a errores en tests de inferencia, como (por ejemplo) al determinar si un coeficiente es significativo.

\subsection{Estimando la varianza del error}

Usando los supuesos de regresión lineal simple obtenemos la varianza de $\hat\beta_0$ y $\hat\beta_1$ indicadas en \eqref{eq:var_hat_beta0} y \eqref{eq:var_hat_beta1}.
El problema es que estas ecuaciones contienen requieren que conozcamos la varianza del error $\sigma^2$, la que casi siempre es desconocida.
Entonces nuestro objetivo ahora es usar nuestros datos para estimar $\sigma^2$, y así poder estimar la varianza de los parámetros.

Usando los supuestos RLS.1 a RLS.5 es posible demostrar que
\begin{equation}
\E(\hat\sigma^2) = \sigma^2.
\end{equation}
Entonces podríamos definir definir el estimador $1/n \sum_{i=1}^n \hat\mu_i^2$ para . Lamentablemente este estimador es sesgado, ya que no toma en cuenta dos restricciones que deben satisfacer los residuos de MCO:
\begin{align*}
\sum_{i=1}^n \hat\mu_i &= 0 \\
\sum_{i=1}^n x_i\hat\mu_i &= 0
\end{align*}
Estas restricciones vienen de las condiciones de primer orden del problema de minimización de MCO planteado en \eqref{eq:prob_min_MCO}.
Una manera de entender cómo influyen estas restricciones al estimador planteado recién es pensando que si conociéramos $n-2$ residuos, siempre podríamos obtener los últimos 2 usando estas restricciones (piensa por qué).
Esto implica que si bien tenemos $n$ grados de libertad para los errores, existen $n-2$ grados de libertad para los residuos de MCO.
Entonces el estimador insesgado de $\sigma^2$ corrige el estimador propuesto recién por este cambio en grados de libertad:
\begin{equation}
\hat\sigma^2 = \frac{1}{n-2} \sum_{i=1}^n \hat \mu_i^2.
\end{equation}
Este estimador a veces es denotado por $S^2$.
Una vez que obtenemos $\hat\sigma^2$ podemos usarlo en las ecuaciones \eqref{eq:var_hat_beta0} y \eqref{eq:var_hat_beta1} para calcular $\Var(\hat\beta_0)$ y $\Var(\hat\beta_1)$.

\subsection{Error estándar (de los estimadores)}

Parece natural pensar que un estimador para la desviación estándar del error es
\begin{equation}
\hat\sigma = \sqrt{\hat\sigma^2}.
\end{equation}
A este término se le conoce como el \kw{error estándar de la regresión}. Lamentablemente $\hat\sigma$ no es un estimador insesgado de $\sigma$, pero es consistente: a medida que la cantidad de datos aumenta, el estimador converge en probabilidad al verdadero parámetro $\sigma$.
Esta situación está representada en la \autoref{fig:estimador_sesgado_consistente}.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
    no markers, domain=0:8, samples=100,
    axis lines*=left, xlabel=$x$, ylabel=$y$,
    every axis y label/.style={at=(current axis.above origin),anchor=south},
    every axis x label/.style={at=(current axis.right of origin),anchor=west},
    height=4cm, width=6cm,
    xtick={6},
    xticklabels={$\sigma$},
    ytick=\empty,
    enlargelimits=false, clip=false, axis on top,
    grid = major
    ]
    \addplot [very thick, magenta!80!black] {gauss(4,1)};
    \addplot [very thick, magenta!50!black] {gauss(5,.5)};
    \addplot [very thick, magenta!20!black] {gauss(5.75,.25)};
    \end{axis}
  \end{tikzpicture}
  \caption{Estimador sesgado consistente}
  \label{fig:estimador_sesgado_consistente}
\end{figure}%

La estimación $\hat\sigma$ es interesante en sí misma porque representa la desviación estándar de $y$ una vez que hemos eliminado los efectos de $x$.
Sin embargo, el principal uso de $\hat\sigma$ es para estimar la desviación estándar de $\hat\beta_1$.
Dado que $\sd(\hat\beta_1) = \sigma / \sqrt{SST_x}$, el estimador natural para esta desviación estándar es
\begin{equation}
\se(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{\sum_{i=1}^n(x_i - \overline x)^2}}.
\end{equation}
A este término se le conoce como el \kw{error estándar} de $\hat\beta_1$.
El error estándar de un estimador nos dá una idea de cuán dispersa es nuestra estimación. Debido a esto, el error estándar juega un rol importante en econometría, y lo usaremos para construir tests estadísticos e intervalos de confianza para distintos métodos que veremos más adelante.


\hrulefill

\subsection{Extraer estadísticos de una regresión}

Para guardar el modelo asignamos \verb|lm()| a un objeto:

<<>>=
modelo <- lm(wage ~ educ, data = ingresos)
@

El objeto \verb|modelo| no solo incluye los coeficientes estimados, si no que una serie de otros elementos que son muy útiles ---veremos ahora por qué. En estricto rigor \verb|modelo| es lo que R llama una lista, y contiene una serie de otros objetos. Podemos imprimir los nombres de estos objetos escribiendo
<<>>=
names(modelo)
@

Para acceder a los objetos guardados usamos la notación de \verb|$|, como es usual en R. Por ejemplo, para imprimir los coeficientes del modelo escribimos
<<>>=
modelo$coefficients
@
También existen funciones para acceder directamente a los objetos. Por ejemplo,
<<>>=
coefficients(modelo)
@
En cualquier caso, el resultado es un vector cuyos elementos tienen nombres: el intercepto ($\beta_0$) es \texttt{(Intercept)} y el nombre del primer coeficiente ($\beta_1$) es el nombre de la variable $x$, es decir, \texttt{educ}. Usando esto podemos acceder a elementos específicos del vector usando sus nombres o sus índices, como se mencionó en [ref]:
<<>>=
coef(modelo)[1]
coef(modelo)["educ"]
@

\subsection{Predicciones y residuos}

La predicción de un modelo, a veces llamada valores ajustados, es simplemente el vector $\hat y_i$. Teniendo los valores ajustados podemos calcular también los residuos del modelo, $\mu_i$:
\begin{align}
\hat y_i &= \hat \beta_0 + \hat\beta_1 x_i \label{eq:prediccion} \\
\hat \mu_i &= y_i - \hat y_i \label{eq:residuos2}
\end{align}
Podemos calcular fácilmente ambos vectores con los elementos que tenemos:
<<>>=
ingresos$prediccion <- coef(modelo)[1] + coef(modelo)[2] * ingresos$educ
ingresos$residuos <- ingresos$wage - ingresos$prediccion
@

Alternativamente, podemos calcular los valores predichos y los residuos del modelo usando funciones específicas para ello:
<<>>=
modelo.sim <- lm(y ~ x, data = simdatos)
simdatos$prediccion <- fitted(modelo.sim)
simdatos$residuos <- resid(modelo.sim)
@

Podemos graficar nuestra predicción sobre los datos para inspeccionar visualmente cómo se ve. Para esto volvemos a usar \paq{ggplot2}:
<<>>=
ggplot(ingresos, aes(educ, wage)) +
  geom_point() +
  geom_line(aes(y = prediccion), color = "blue")
@

También resulta interesante analizar los residuos, que son ``la otra cara de la moneda''. Si bien los valores predichos nos muestran el patrón que el modelo ha sido capaz de capturar, los residuos nos mostrarán lo que el modelo ha omitido. Recordar que los residuos son simplemente la diferencia entre el valor predicho y el real, como se indica en \eqref{eq:residuos}. Graficamos los residuos escribiendo

<<>>=
ggplot(ingresos, aes(educ, residuos)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_point()
@

<<>>=
ggplot(simdatos, aes(x, residuos)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_point()
@

Además de estos elementos directamente accesibles guardados en \verb|modelo|, existen una serie de funciones que podemos aplicar al objeto para realizar otros cálculos o resumir información útil. Por ejemplo, es muy común usar \verb|summary()| para obtener un resumen de información importante del modelo estimado:
<<>>=
summary(modelo)
@